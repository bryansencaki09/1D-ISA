{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de3619b5-6147-47c5-91e1-b0d0cb29b0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 15:49:35.210009: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.20.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import sigmoid\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, LayerNormalization, Bidirectional, LSTM, GRU, Layer, SpatialDropout1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Lambda, Reshape, Flatten, Input, MultiHeadAttention, Flatten, Concatenate, Add, Multiply, Embedding\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc75895-4609-4e5e-9b5a-117bae7b23ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os, math, random\n",
    "import numpy as np\n",
    "import pyrsgis\n",
    "from pyrsgis import raster\n",
    "from osgeo import gdal\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy import interpolate\n",
    "import rasterio\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bad2eb7c-cc26-4eae-afee-3a68fb85598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cc1fbab-dbe2-477d-ab41-f6103b22c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiFocalLossSlopeElevationConstraint(tf.keras.losses.Loss):\n",
    "    def __init__(self, slope_threshold=40, elevation_threshold=2000,\n",
    "                 alpha=0.25, gamma=2.0,\n",
    "                 lambda_slope=0.4, lambda_elevation=0.3,\n",
    "                 name='multi_focal_loss_slope_elevation_constraint'):\n",
    "        super().__init__(name=name)\n",
    "        self.slope_threshold = slope_threshold\n",
    "        self.elevation_threshold = elevation_threshold\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.lambda_slope = lambda_slope\n",
    "        self.lambda_elevation = lambda_elevation\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        return tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            'slope_threshold': self.slope_threshold,\n",
    "            'elevation_threshold': self.elevation_threshold,\n",
    "            'alpha': self.alpha,\n",
    "            'gamma': self.gamma,\n",
    "            'lambda_slope': self.lambda_slope,\n",
    "            'lambda_elevation': self.lambda_elevation\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93379ce1-2498-4a96-98e3-6b7528cd4d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALPELayer(Layer):\n",
    "    \"\"\"\n",
    "    Attention-based Learnable Positional Encoding as a custom Keras Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, n_timesteps=24, embed_dim=64, dropout_rate=0.5, name='alpe', **kwargs):\n",
    "        super(ALPELayer, self).__init__(name=name, **kwargs)\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        Build layer components\n",
    "        input_shape: tuple of (x_shape, mask_shape)\n",
    "        \"\"\"\n",
    "        feature_dim = input_shape[0][-1]  # Get feature dimension from x\n",
    "        \n",
    "        # Positional embedding layer\n",
    "        self.pos_embedding = Embedding(\n",
    "            input_dim=self.n_timesteps,\n",
    "            output_dim=self.embed_dim,\n",
    "            name=f'{self.name}_pos_embed')\n",
    "        \n",
    "        # 1D Convolution for learning interpolation patterns\n",
    "        self.conv1d = Conv1D(\n",
    "            filters=self.embed_dim,\n",
    "            kernel_size=3,\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "            name=f'{self.name}_conv1d')\n",
    "        \n",
    "        # ECA: Adaptive kernel size\n",
    "        k_size = max(3, int(abs((np.log2(self.embed_dim) + 1) / 2)))\n",
    "        if k_size % 2 == 0:\n",
    "            k_size += 1\n",
    "        \n",
    "        # ECA channel attention\n",
    "        self.eca_conv = Conv1D(\n",
    "            filters=1,\n",
    "            kernel_size=k_size,\n",
    "            padding='same',\n",
    "            activation='sigmoid',\n",
    "            name=f'{self.name}_eca')\n",
    "        \n",
    "        # Projection to match input feature dimension\n",
    "        if self.embed_dim != feature_dim:\n",
    "            self.projection = Dense(\n",
    "                units=feature_dim,\n",
    "                name=f'{self.name}_projection')\n",
    "        else:\n",
    "            self.projection = None\n",
    "\n",
    "        # Add dropout layer\n",
    "        self.dropout = Dropout(self.dropout_rate, name=f'{self.name}_dropout')\n",
    "        \n",
    "        # Store feature_dim for rebuild\n",
    "        self.feature_dim = feature_dim\n",
    "        \n",
    "        # CRITICAL: Actually build the sub-layers\n",
    "        self.pos_embedding.build((None,))  # Embedding input shape\n",
    "        self.conv1d.build((None, self.n_timesteps, self.embed_dim))\n",
    "        \n",
    "        k_size = max(3, int(abs((np.log2(self.embed_dim) + 1) / 2)))\n",
    "        if k_size % 2 == 0:\n",
    "            k_size += 1\n",
    "        self.eca_conv.build((None, 1, self.embed_dim))\n",
    "        \n",
    "        if self.projection is not None:\n",
    "            self.projection.build((None, self.n_timesteps, self.embed_dim))\n",
    "        \n",
    "        self.dropout.build((None, self.n_timesteps, feature_dim))\n",
    "        \n",
    "        super(ALPELayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        inputs: [x, confidence_mask]\n",
    "            x: [batch, timesteps, features]\n",
    "            confidence_mask: [batch, timesteps]\n",
    "        \"\"\"\n",
    "        x, confidence_mask = inputs\n",
    "        \n",
    "        # ============================================================\n",
    "        # STEP 1: Create positional embeddings\n",
    "        # ============================================================\n",
    "        position_ids = tf.range(self.n_timesteps)\n",
    "        pos_embedding = self.pos_embedding(position_ids)  # [timesteps, embed_dim]\n",
    "        \n",
    "        # Broadcast to batch dimension\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        pos_embedding = tf.tile(\n",
    "            tf.expand_dims(pos_embedding, axis=0),\n",
    "            [batch_size, 1, 1]\n",
    "        )  # [batch, timesteps, embed_dim]\n",
    "        \n",
    "        # Apply conf mask\n",
    "        mask_expanded = tf.expand_dims(confidence_mask, axis=-1)  # [batch, timesteps, 1]\n",
    "        mask_expanded = tf.cast(mask_expanded, tf.float32)\n",
    "        \n",
    "        # Invert mask: confidence=1.0 → weight=0.0, confidence=0.0 → weight=1.0\n",
    "        alpe_weight = 1.0 - mask_expanded  # [batch, timesteps, 1]\n",
    "        \n",
    "        # Apply weight to positional encoding\n",
    "        masked_pe = pos_embedding * alpe_weight  # [batch, timesteps, embed_dim]\n",
    "\n",
    "        # Keras Conv1D expects [batch, timesteps, channels]\n",
    "        # masked_pe is already in correct shape: [batch, timesteps, embed_dim]\n",
    "        pe_conv = self.conv1d(masked_pe)  # [batch, timesteps, embed_dim]\n",
    "        \n",
    "        # For ECA on channels, we need to work on channel dimension\n",
    "        # Transpose to [batch, embed_dim, timesteps] for channel-wise operations\n",
    "        pe_transposed = tf.transpose(pe_conv, perm=[0, 2, 1])  # [batch, embed_dim, timesteps]\n",
    "        \n",
    "        # Global average pooling across time\n",
    "        gap = tf.reduce_mean(pe_transposed, axis=-1, keepdims=True)  # [batch, embed_dim, 1]\n",
    "        \n",
    "        # Reshape for Conv1D: [batch, embed_dim, 1] → need [batch, timesteps=embed_dim, channels=1]\n",
    "        gap_reshaped = tf.transpose(gap, perm=[0, 2, 1])  # [batch, 1, embed_dim]\n",
    "        \n",
    "        # Channel attention via 1D conv\n",
    "        channel_att = self.eca_conv(gap_reshaped)  # [batch, 1, embed_dim]\n",
    "        \n",
    "        # Reshape back: [batch, 1, embed_dim] → [batch, embed_dim, 1]\n",
    "        channel_att = tf.transpose(channel_att, perm=[0, 2, 1])  # [batch, embed_dim, 1]\n",
    "        \n",
    "        # Apply attention to pe_transposed [batch, embed_dim, timesteps]\n",
    "        pe_attended = pe_transposed * channel_att  # [batch, embed_dim, timesteps]\n",
    "        \n",
    "        # ============================================================\n",
    "        # STEP 5: Transpose back to [batch, timesteps, embed_dim]\n",
    "        # ============================================================\n",
    "        alpe_output = tf.transpose(pe_attended, perm=[0, 2, 1])  # [batch, timesteps, embed_dim]\n",
    "        \n",
    "        # Project to match input feature dimension if needed\n",
    "        if self.projection is not None:\n",
    "            alpe_output = self.projection(alpe_output)  # [batch, timesteps, features]\n",
    "\n",
    "        alpe_output = self.dropout(alpe_output, training=training)\n",
    "        \n",
    "        # ============================================================\n",
    "        # STEP 6: Residual connection with weighting\n",
    "        # ============================================================\n",
    "        # Now shapes match:\n",
    "        # alpe_output: [batch, timesteps, features]\n",
    "        # alpe_weight: [batch, timesteps, 1]\n",
    "        alpe_contribution = alpe_output * alpe_weight  # Broadcasting works!\n",
    "        x_enhanced = x + alpe_contribution\n",
    "        \n",
    "        return x_enhanced\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]  # Return x's shape\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(ALPELayer, self).get_config()\n",
    "        config.update({\n",
    "            'n_timesteps': self.n_timesteps,\n",
    "            'embed_dim': self.embed_dim,\n",
    "            'dropout_rate': self.dropout_rate\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    def get_build_config(self):\n",
    "        \"\"\"Return configuration needed to rebuild the layer during loading\"\"\"\n",
    "        build_config = {\n",
    "            'feature_dim': getattr(self, 'feature_dim', None)\n",
    "        }\n",
    "        return build_config\n",
    "    \n",
    "    def build_from_config(self, config):\n",
    "        \"\"\"Rebuild the layer's internal state from config\"\"\"\n",
    "        feature_dim = config.get('feature_dim')\n",
    "        if feature_dim is not None:\n",
    "            # Reconstruct input_shape that build() expects\n",
    "            input_shape = (\n",
    "                (None, self.n_timesteps, feature_dim),  # x_shape\n",
    "                (None, self.n_timesteps)  # mask_shape\n",
    "            )\n",
    "            self.build(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "148f3ba0-8c76-4355-907d-c01789644143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1766130579.597022  474247 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1088 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model_path = '/home/jupyter-bryan/ISA_Data/ISA_Citarum_Multi_Orig_withALPE.keras'\n",
    "model = load_model(model_path,\n",
    "                   custom_objects={'ALPELayer': ALPELayer,\n",
    "                                   'MultiFocalLossSlopeElevationConstraint': MultiFocalLossSlopeElevationConstraint},\n",
    "                   compile=False)\n",
    "\n",
    "# Then recompile with your custom loss\n",
    "# model.compile(\n",
    "#     optimizer=Adam(learning_rate=0.001),\n",
    "#     loss=MultiFocalLossSlopeElevationConstraint(),\n",
    "#     metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "caa740c1-f1fb-47b4-8053-0c3282f82f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('/home/jupyter-bryan/ISA_Data/ISA_Citarum_Multi_Orig_CCEL.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20989a8e-aca6-457c-8e8e-5cb13e8fb237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: PROJ: proj_create_from_database: Open of /opt/tljh/user/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /opt/tljh/user/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /opt/tljh/user/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /opt/tljh/user/share/proj failed\n",
      "Warning 1: PROJ: proj_create_from_database: Open of /opt/tljh/user/share/proj failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 3147, 3194)\n",
      "(24, 3147, 3194)\n",
      "(24, 3147, 3194)\n",
      "(24, 3147, 3194)\n",
      "(24, 3147, 3194)\n",
      "(24, 3147, 3194)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning 1: PROJ: proj_create_from_database: Open of /opt/tljh/user/share/proj failed\n"
     ]
    }
   ],
   "source": [
    "# # Load the saved model\n",
    "# model_path = '/home/jupyter-bryan/ISA_Data/ISA_Citarum_Multi_Orig.keras'\n",
    "\n",
    "# model = load_model(model_path,\n",
    "#                    custom_objects={'MultiFocalLossSlopeElevationConstraint': MultiFocalLossSlopeElevationConstraint},\n",
    "#                    compile=False)\n",
    "\n",
    "# Load a new multispectral image\n",
    "ds_ndvi, img_ndvi  = raster.read('/home/jupyter-bryan/ISA_Data/Raster/DKI_3-Month_NDVI_2019_2024.tif')\n",
    "ds_mndwi, img_mndwi  = raster.read('/home/jupyter-bryan/ISA_Data/Raster/DKI_3-Month_MNDWI_2019_2024.tif')\n",
    "ds_ndbi, img_ndbi  = raster.read('/home/jupyter-bryan/ISA_Data/Raster/DKI_3-Month_NDBI_2019_2024.tif')\n",
    "ds_ndbsi, img_ndbsi  = raster.read('/home/jupyter-bryan/ISA_Data/Raster/DKI_3-Month_NDBSI_2019_2024.tif')\n",
    "ds_cbi, img_cbi  = raster.read('/home/jupyter-bryan/ISA_Data/Raster/DKI_3-Month_CBI_2019_2024.tif')\n",
    "ds_uci, img_uci  = raster.read('/home/jupyter-bryan/ISA_Data/Raster/DKI_3-Month_UCI_2019_2024.tif')\n",
    "\n",
    "print(img_ndvi.shape)\n",
    "print(img_mndwi.shape)\n",
    "print(img_ndbi.shape)\n",
    "print(img_ndbsi.shape)\n",
    "print(img_cbi.shape)\n",
    "print(img_uci.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24ea886d-af11-4c6a-b0f9-da6c498e64f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ndvi_reshape = img_ndvi.reshape(img_ndvi.shape[0],(img_ndvi.shape[1]*img_ndvi.shape[2])).T\n",
    "img_mndwi_reshape = img_mndwi.reshape(img_mndwi.shape[0],(img_mndwi.shape[1]*img_mndwi.shape[2])).T\n",
    "img_ndbi_reshape = img_ndbi.reshape(img_ndbi.shape[0],(img_ndbi.shape[1]*img_ndbi.shape[2])).T\n",
    "img_ndbsi_reshape = img_ndbsi.reshape(img_ndbsi.shape[0],(img_ndbsi.shape[1]*img_ndbsi.shape[2])).T\n",
    "img_cbi_reshape = img_cbi.reshape(img_cbi.shape[0],(img_cbi.shape[1]*img_cbi.shape[2])).T\n",
    "img_uci_reshape = img_uci.reshape(img_uci.shape[0],(img_uci.shape[1]*img_uci.shape[2])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08150023-ba41-4e07-bfd5-e3fdd2f62388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_interpolate_with_mask(data):\n",
    "    \"\"\"\n",
    "    Interpolate missing data AND create confidence mask\n",
    "    Returns: (interpolated_data, confidence_mask)\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(data, dtype=np.float32)\n",
    "    \n",
    "    # Track original missing values BEFORE interpolation\n",
    "    original_missing_mask = df.isnull()  # True = missing\n",
    "    \n",
    "    if len(df) > 100000:\n",
    "        print(f\"Processing large dataset with {len(df):,} pixels in chunks...\")\n",
    "        chunk_size = 50000\n",
    "        results = []\n",
    "        masks = []\n",
    "        \n",
    "        for i in range(0, len(df), chunk_size):\n",
    "            chunk = df.iloc[i:i+chunk_size].copy()\n",
    "            chunk_missing = original_missing_mask.iloc[i:i+chunk_size].copy()\n",
    "            \n",
    "            # Interpolate\n",
    "            chunk_linear = chunk.interpolate(method='linear', limit_direction='both', axis=1, limit=None)\n",
    "            chunk_spline = chunk_linear.interpolate(method='spline', order=3, axis=1)\n",
    "            chunk_filled = chunk_spline.fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
    "            \n",
    "            # Identify what STILL missing after interpolation\n",
    "            still_missing = chunk_filled.isnull()\n",
    "            \n",
    "            # Fill remaining with zeros\n",
    "            chunk_final = chunk_filled.fillna(0)\n",
    "            chunk_clipped = chunk_final.clip(-1e6, 1e6)\n",
    "            \n",
    "            # Create confidence mask\n",
    "            # 1.0 = valid or successfully interpolated\n",
    "            # 0.0 = zero-filled (ALPE will handle)\n",
    "            chunk_mask = np.ones_like(chunk_clipped.values, dtype=np.float32)\n",
    "            chunk_mask[still_missing.values] = 0.0\n",
    "            \n",
    "            results.append(chunk_clipped.values.astype(np.float32))\n",
    "            masks.append(chunk_mask)\n",
    "            \n",
    "            print(f\"Processed chunk {i//chunk_size + 1}/{(len(df)-1)//chunk_size + 1}\")\n",
    "            gc.collect()\n",
    "        \n",
    "        valid_result = np.vstack(results)\n",
    "        confidence_mask = np.vstack(masks)\n",
    "    else:\n",
    "        # Small dataset processing\n",
    "        df_linear = df.interpolate(method='linear', limit_direction='both', axis=1, limit=None)\n",
    "        df_spline = df_linear.interpolate(method='spline', order=3, axis=1)\n",
    "        df_filled = df_spline.fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
    "        \n",
    "        still_missing = df_filled.isnull()\n",
    "        df_final = df_filled.fillna(0)\n",
    "        valid_result = df_final.clip(-1e6, 1e6).values.astype(np.float32)\n",
    "        \n",
    "        # Create mask\n",
    "        confidence_mask = np.ones_like(valid_result, dtype=np.float32)\n",
    "        confidence_mask[still_missing.values] = 0.0\n",
    "    \n",
    "    print(f\"  Pixels with zero-filled data: {(confidence_mask == 0.0).any(axis=1).sum()}\")\n",
    "    \n",
    "    return valid_result, confidence_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65d05a17-0e30-48fd-9f40-7d224f072bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_and_interpolate(data):\n",
    "#     df = pd.DataFrame(data, dtype=np.float32) \n",
    "#     df.dropna(how='all')\n",
    "    \n",
    "#     if len(df) > 100000:  \n",
    "#         print(f\"Processing large dataset with {len(df):,} pixels in chunks...\")\n",
    "#         chunk_size = 50000\n",
    "#         results = []\n",
    "#         for i in range(0, len(df), chunk_size):\n",
    "#             chunk = df.iloc[i:i+chunk_size].copy()\n",
    "#             chunk_linear = chunk.interpolate(method='linear', limit_direction='both', axis=1, limit=None)\n",
    "#             chunk_final = chunk_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
    "#             chunk_clipped = chunk_final.clip(-1e6, 1e6)\n",
    "#             results.append(chunk_clipped.values.astype(np.float32))\n",
    "#             print(f\"Processed chunk {i//chunk_size + 1}/{(len(df)-1)//chunk_size + 1}\")\n",
    "#             gc.collect()\n",
    "#         valid_result = np.vstack(results)\n",
    "#     else:\n",
    "#         df_linear = df.interpolate(method='linear', limit_direction='both', axis=1, limit=None)\n",
    "#         df_final = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
    "#         valid_result = df_final.clip(-1e6, 1e6).values.astype(np.float32)\n",
    "\n",
    "#     return valid_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1d7976f-acd1-4799-b1e5-bc9ce7890efb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# print(\"\\n=== Interpolating NDVI ===\")\n",
    "# img_ndvi_int = clean_and_interpolate(img_ndvi_reshape)\n",
    "\n",
    "# print(\"\\n=== Interpolating MNDWI ===\")\n",
    "# img_mndwi_int = clean_and_interpolate(img_mndwi_reshape)\n",
    "\n",
    "# print(\"\\n=== Interpolating NDBI ===\")\n",
    "# img_ndbi_int = clean_and_interpolate(img_ndbi_reshape)\n",
    "\n",
    "# print(\"\\n=== Interpolating NDBSI ===\")\n",
    "# img_ndbsi_int = clean_and_interpolate(img_ndbsi_reshape)\n",
    "\n",
    "# print(\"\\n=== Interpolating CBI ===\")\n",
    "# img_cbi_int = clean_and_interpolate(img_cbi_reshape)\n",
    "\n",
    "# print(\"\\n=== Interpolating UCI ===\")\n",
    "# img_uci_int = clean_and_interpolate(img_uci_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b03b8dc-8ee6-4b8b-93d6-dbcbed0bf849",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Interpolating NDVI ===\n",
      "Processing large dataset with 10,051,518 pixels in chunks...\n",
      "Processed chunk 1/202\n",
      "Processed chunk 2/202\n",
      "Processed chunk 3/202\n",
      "Processed chunk 4/202\n",
      "Processed chunk 5/202\n",
      "Processed chunk 6/202\n",
      "Processed chunk 7/202\n",
      "Processed chunk 8/202\n",
      "Processed chunk 9/202\n",
      "Processed chunk 10/202\n",
      "Processed chunk 11/202\n",
      "Processed chunk 12/202\n",
      "Processed chunk 13/202\n",
      "Processed chunk 14/202\n",
      "Processed chunk 15/202\n",
      "Processed chunk 16/202\n",
      "Processed chunk 17/202\n",
      "Processed chunk 18/202\n",
      "Processed chunk 19/202\n",
      "Processed chunk 20/202\n",
      "Processed chunk 21/202\n",
      "Processed chunk 22/202\n",
      "Processed chunk 23/202\n",
      "Processed chunk 24/202\n",
      "Processed chunk 25/202\n",
      "Processed chunk 26/202\n",
      "Processed chunk 27/202\n",
      "Processed chunk 28/202\n",
      "Processed chunk 29/202\n",
      "Processed chunk 30/202\n",
      "Processed chunk 31/202\n",
      "Processed chunk 32/202\n",
      "Processed chunk 33/202\n",
      "Processed chunk 34/202\n",
      "Processed chunk 35/202\n",
      "Processed chunk 36/202\n",
      "Processed chunk 37/202\n",
      "Processed chunk 38/202\n",
      "Processed chunk 39/202\n",
      "Processed chunk 40/202\n",
      "Processed chunk 41/202\n",
      "Processed chunk 42/202\n",
      "Processed chunk 43/202\n",
      "Processed chunk 44/202\n",
      "Processed chunk 45/202\n",
      "Processed chunk 46/202\n",
      "Processed chunk 47/202\n",
      "Processed chunk 48/202\n",
      "Processed chunk 49/202\n",
      "Processed chunk 50/202\n",
      "Processed chunk 51/202\n",
      "Processed chunk 52/202\n",
      "Processed chunk 53/202\n",
      "Processed chunk 54/202\n",
      "Processed chunk 55/202\n",
      "Processed chunk 56/202\n",
      "Processed chunk 57/202\n",
      "Processed chunk 58/202\n",
      "Processed chunk 59/202\n",
      "Processed chunk 60/202\n",
      "Processed chunk 61/202\n",
      "Processed chunk 62/202\n",
      "Processed chunk 63/202\n",
      "Processed chunk 64/202\n",
      "Processed chunk 65/202\n",
      "Processed chunk 66/202\n",
      "Processed chunk 67/202\n",
      "Processed chunk 68/202\n",
      "Processed chunk 69/202\n",
      "Processed chunk 70/202\n",
      "Processed chunk 71/202\n",
      "Processed chunk 72/202\n",
      "Processed chunk 73/202\n",
      "Processed chunk 74/202\n",
      "Processed chunk 75/202\n",
      "Processed chunk 76/202\n",
      "Processed chunk 77/202\n",
      "Processed chunk 78/202\n",
      "Processed chunk 79/202\n",
      "Processed chunk 80/202\n",
      "Processed chunk 81/202\n",
      "Processed chunk 82/202\n",
      "Processed chunk 83/202\n",
      "Processed chunk 84/202\n",
      "Processed chunk 85/202\n",
      "Processed chunk 86/202\n",
      "Processed chunk 87/202\n",
      "Processed chunk 88/202\n",
      "Processed chunk 89/202\n",
      "Processed chunk 90/202\n",
      "Processed chunk 91/202\n",
      "Processed chunk 92/202\n",
      "Processed chunk 93/202\n",
      "Processed chunk 94/202\n",
      "Processed chunk 95/202\n",
      "Processed chunk 96/202\n",
      "Processed chunk 97/202\n",
      "Processed chunk 98/202\n",
      "Processed chunk 99/202\n",
      "Processed chunk 100/202\n",
      "Processed chunk 101/202\n",
      "Processed chunk 102/202\n",
      "Processed chunk 103/202\n",
      "Processed chunk 104/202\n",
      "Processed chunk 105/202\n",
      "Processed chunk 106/202\n",
      "Processed chunk 107/202\n",
      "Processed chunk 108/202\n",
      "Processed chunk 109/202\n",
      "Processed chunk 110/202\n",
      "Processed chunk 111/202\n",
      "Processed chunk 112/202\n",
      "Processed chunk 113/202\n",
      "Processed chunk 114/202\n",
      "Processed chunk 115/202\n",
      "Processed chunk 116/202\n",
      "Processed chunk 117/202\n",
      "Processed chunk 118/202\n",
      "Processed chunk 119/202\n",
      "Processed chunk 120/202\n",
      "Processed chunk 121/202\n",
      "Processed chunk 122/202\n",
      "Processed chunk 123/202\n",
      "Processed chunk 124/202\n",
      "Processed chunk 125/202\n",
      "Processed chunk 126/202\n",
      "Processed chunk 127/202\n",
      "Processed chunk 128/202\n",
      "Processed chunk 129/202\n",
      "Processed chunk 130/202\n",
      "Processed chunk 131/202\n",
      "Processed chunk 132/202\n",
      "Processed chunk 133/202\n",
      "Processed chunk 134/202\n",
      "Processed chunk 135/202\n",
      "Processed chunk 136/202\n",
      "Processed chunk 137/202\n",
      "Processed chunk 138/202\n",
      "Processed chunk 139/202\n",
      "Processed chunk 140/202\n",
      "Processed chunk 141/202\n",
      "Processed chunk 142/202\n",
      "Processed chunk 143/202\n",
      "Processed chunk 144/202\n",
      "Processed chunk 145/202\n",
      "Processed chunk 146/202\n",
      "Processed chunk 147/202\n",
      "Processed chunk 148/202\n",
      "Processed chunk 149/202\n",
      "Processed chunk 150/202\n",
      "Processed chunk 151/202\n",
      "Processed chunk 152/202\n",
      "Processed chunk 153/202\n",
      "Processed chunk 154/202\n",
      "Processed chunk 155/202\n",
      "Processed chunk 156/202\n",
      "Processed chunk 157/202\n",
      "Processed chunk 158/202\n",
      "Processed chunk 159/202\n",
      "Processed chunk 160/202\n",
      "Processed chunk 161/202\n",
      "Processed chunk 162/202\n",
      "Processed chunk 163/202\n",
      "Processed chunk 164/202\n",
      "Processed chunk 165/202\n",
      "Processed chunk 166/202\n",
      "Processed chunk 167/202\n",
      "Processed chunk 168/202\n",
      "Processed chunk 169/202\n",
      "Processed chunk 170/202\n",
      "Processed chunk 171/202\n",
      "Processed chunk 172/202\n",
      "Processed chunk 173/202\n",
      "Processed chunk 174/202\n",
      "Processed chunk 175/202\n",
      "Processed chunk 176/202\n",
      "Processed chunk 177/202\n",
      "Processed chunk 178/202\n",
      "Processed chunk 179/202\n",
      "Processed chunk 180/202\n",
      "Processed chunk 181/202\n",
      "Processed chunk 182/202\n",
      "Processed chunk 183/202\n",
      "Processed chunk 184/202\n",
      "Processed chunk 185/202\n",
      "Processed chunk 186/202\n",
      "Processed chunk 187/202\n",
      "Processed chunk 188/202\n",
      "Processed chunk 189/202\n",
      "Processed chunk 190/202\n",
      "Processed chunk 191/202\n",
      "Processed chunk 192/202\n",
      "Processed chunk 193/202\n",
      "Processed chunk 194/202\n",
      "Processed chunk 195/202\n",
      "Processed chunk 196/202\n",
      "Processed chunk 197/202\n",
      "Processed chunk 198/202\n",
      "Processed chunk 199/202\n",
      "Processed chunk 200/202\n",
      "Processed chunk 201/202\n",
      "Processed chunk 202/202\n",
      "  Pixels with zero-filled data: 3161\n",
      "\n",
      "=== Interpolating MNDWI ===\n",
      "Processing large dataset with 10,051,518 pixels in chunks...\n",
      "Processed chunk 1/202\n",
      "Processed chunk 2/202\n",
      "Processed chunk 3/202\n",
      "Processed chunk 4/202\n",
      "Processed chunk 5/202\n",
      "Processed chunk 6/202\n",
      "Processed chunk 7/202\n",
      "Processed chunk 8/202\n",
      "Processed chunk 9/202\n",
      "Processed chunk 10/202\n",
      "Processed chunk 11/202\n",
      "Processed chunk 12/202\n",
      "Processed chunk 13/202\n",
      "Processed chunk 14/202\n",
      "Processed chunk 15/202\n",
      "Processed chunk 16/202\n",
      "Processed chunk 17/202\n",
      "Processed chunk 18/202\n",
      "Processed chunk 19/202\n",
      "Processed chunk 20/202\n",
      "Processed chunk 21/202\n",
      "Processed chunk 22/202\n",
      "Processed chunk 23/202\n",
      "Processed chunk 24/202\n",
      "Processed chunk 25/202\n",
      "Processed chunk 26/202\n",
      "Processed chunk 27/202\n",
      "Processed chunk 28/202\n",
      "Processed chunk 29/202\n",
      "Processed chunk 30/202\n",
      "Processed chunk 31/202\n",
      "Processed chunk 32/202\n",
      "Processed chunk 33/202\n",
      "Processed chunk 34/202\n",
      "Processed chunk 35/202\n",
      "Processed chunk 36/202\n",
      "Processed chunk 37/202\n",
      "Processed chunk 38/202\n",
      "Processed chunk 39/202\n",
      "Processed chunk 40/202\n",
      "Processed chunk 41/202\n",
      "Processed chunk 42/202\n",
      "Processed chunk 43/202\n",
      "Processed chunk 44/202\n",
      "Processed chunk 45/202\n",
      "Processed chunk 46/202\n",
      "Processed chunk 47/202\n",
      "Processed chunk 48/202\n",
      "Processed chunk 49/202\n",
      "Processed chunk 50/202\n",
      "Processed chunk 51/202\n",
      "Processed chunk 52/202\n",
      "Processed chunk 53/202\n",
      "Processed chunk 54/202\n",
      "Processed chunk 55/202\n",
      "Processed chunk 56/202\n",
      "Processed chunk 57/202\n",
      "Processed chunk 58/202\n",
      "Processed chunk 59/202\n",
      "Processed chunk 60/202\n",
      "Processed chunk 61/202\n",
      "Processed chunk 62/202\n",
      "Processed chunk 63/202\n",
      "Processed chunk 64/202\n",
      "Processed chunk 65/202\n",
      "Processed chunk 66/202\n",
      "Processed chunk 67/202\n",
      "Processed chunk 68/202\n",
      "Processed chunk 69/202\n",
      "Processed chunk 70/202\n",
      "Processed chunk 71/202\n",
      "Processed chunk 72/202\n",
      "Processed chunk 73/202\n",
      "Processed chunk 74/202\n",
      "Processed chunk 75/202\n",
      "Processed chunk 76/202\n",
      "Processed chunk 77/202\n",
      "Processed chunk 78/202\n",
      "Processed chunk 79/202\n",
      "Processed chunk 80/202\n",
      "Processed chunk 81/202\n",
      "Processed chunk 82/202\n",
      "Processed chunk 83/202\n",
      "Processed chunk 84/202\n",
      "Processed chunk 85/202\n",
      "Processed chunk 86/202\n",
      "Processed chunk 87/202\n",
      "Processed chunk 88/202\n",
      "Processed chunk 89/202\n",
      "Processed chunk 90/202\n",
      "Processed chunk 91/202\n",
      "Processed chunk 92/202\n",
      "Processed chunk 93/202\n",
      "Processed chunk 94/202\n",
      "Processed chunk 95/202\n",
      "Processed chunk 96/202\n",
      "Processed chunk 97/202\n",
      "Processed chunk 98/202\n",
      "Processed chunk 99/202\n",
      "Processed chunk 100/202\n",
      "Processed chunk 101/202\n",
      "Processed chunk 102/202\n",
      "Processed chunk 103/202\n",
      "Processed chunk 104/202\n",
      "Processed chunk 105/202\n",
      "Processed chunk 106/202\n",
      "Processed chunk 107/202\n",
      "Processed chunk 108/202\n",
      "Processed chunk 109/202\n",
      "Processed chunk 110/202\n",
      "Processed chunk 111/202\n",
      "Processed chunk 112/202\n",
      "Processed chunk 113/202\n",
      "Processed chunk 114/202\n",
      "Processed chunk 115/202\n",
      "Processed chunk 116/202\n",
      "Processed chunk 117/202\n",
      "Processed chunk 118/202\n",
      "Processed chunk 119/202\n",
      "Processed chunk 120/202\n",
      "Processed chunk 121/202\n",
      "Processed chunk 122/202\n",
      "Processed chunk 123/202\n",
      "Processed chunk 124/202\n",
      "Processed chunk 125/202\n",
      "Processed chunk 126/202\n",
      "Processed chunk 127/202\n",
      "Processed chunk 128/202\n",
      "Processed chunk 129/202\n",
      "Processed chunk 130/202\n",
      "Processed chunk 131/202\n",
      "Processed chunk 132/202\n",
      "Processed chunk 133/202\n",
      "Processed chunk 134/202\n",
      "Processed chunk 135/202\n",
      "Processed chunk 136/202\n",
      "Processed chunk 137/202\n",
      "Processed chunk 138/202\n",
      "Processed chunk 139/202\n",
      "Processed chunk 140/202\n",
      "Processed chunk 141/202\n",
      "Processed chunk 142/202\n",
      "Processed chunk 143/202\n",
      "Processed chunk 144/202\n",
      "Processed chunk 145/202\n",
      "Processed chunk 146/202\n",
      "Processed chunk 147/202\n",
      "Processed chunk 148/202\n",
      "Processed chunk 149/202\n",
      "Processed chunk 150/202\n",
      "Processed chunk 151/202\n",
      "Processed chunk 152/202\n",
      "Processed chunk 153/202\n",
      "Processed chunk 154/202\n",
      "Processed chunk 155/202\n",
      "Processed chunk 156/202\n",
      "Processed chunk 157/202\n",
      "Processed chunk 158/202\n",
      "Processed chunk 159/202\n",
      "Processed chunk 160/202\n",
      "Processed chunk 161/202\n",
      "Processed chunk 162/202\n",
      "Processed chunk 163/202\n",
      "Processed chunk 164/202\n",
      "Processed chunk 165/202\n",
      "Processed chunk 166/202\n",
      "Processed chunk 167/202\n",
      "Processed chunk 168/202\n",
      "Processed chunk 169/202\n",
      "Processed chunk 170/202\n",
      "Processed chunk 171/202\n",
      "Processed chunk 172/202\n",
      "Processed chunk 173/202\n",
      "Processed chunk 174/202\n",
      "Processed chunk 175/202\n",
      "Processed chunk 176/202\n",
      "Processed chunk 177/202\n",
      "Processed chunk 178/202\n",
      "Processed chunk 179/202\n",
      "Processed chunk 180/202\n",
      "Processed chunk 181/202\n",
      "Processed chunk 182/202\n",
      "Processed chunk 183/202\n",
      "Processed chunk 184/202\n",
      "Processed chunk 185/202\n",
      "Processed chunk 186/202\n",
      "Processed chunk 187/202\n",
      "Processed chunk 188/202\n",
      "Processed chunk 189/202\n",
      "Processed chunk 190/202\n",
      "Processed chunk 191/202\n",
      "Processed chunk 192/202\n",
      "Processed chunk 193/202\n",
      "Processed chunk 194/202\n",
      "Processed chunk 195/202\n",
      "Processed chunk 196/202\n",
      "Processed chunk 197/202\n",
      "Processed chunk 198/202\n",
      "Processed chunk 199/202\n",
      "Processed chunk 200/202\n",
      "Processed chunk 201/202\n",
      "Processed chunk 202/202\n",
      "  Pixels with zero-filled data: 3161\n",
      "\n",
      "=== Interpolating NDBI ===\n",
      "Processing large dataset with 10,051,518 pixels in chunks...\n",
      "Processed chunk 1/202\n",
      "Processed chunk 2/202\n",
      "Processed chunk 3/202\n",
      "Processed chunk 4/202\n",
      "Processed chunk 5/202\n",
      "Processed chunk 6/202\n",
      "Processed chunk 7/202\n",
      "Processed chunk 8/202\n",
      "Processed chunk 9/202\n",
      "Processed chunk 10/202\n",
      "Processed chunk 11/202\n",
      "Processed chunk 12/202\n",
      "Processed chunk 13/202\n",
      "Processed chunk 14/202\n",
      "Processed chunk 15/202\n",
      "Processed chunk 16/202\n",
      "Processed chunk 17/202\n",
      "Processed chunk 18/202\n",
      "Processed chunk 19/202\n",
      "Processed chunk 20/202\n",
      "Processed chunk 21/202\n",
      "Processed chunk 22/202\n",
      "Processed chunk 23/202\n",
      "Processed chunk 24/202\n",
      "Processed chunk 25/202\n",
      "Processed chunk 26/202\n",
      "Processed chunk 27/202\n",
      "Processed chunk 28/202\n",
      "Processed chunk 29/202\n",
      "Processed chunk 30/202\n",
      "Processed chunk 31/202\n",
      "Processed chunk 32/202\n",
      "Processed chunk 33/202\n",
      "Processed chunk 34/202\n",
      "Processed chunk 35/202\n",
      "Processed chunk 36/202\n",
      "Processed chunk 37/202\n",
      "Processed chunk 38/202\n",
      "Processed chunk 39/202\n",
      "Processed chunk 40/202\n",
      "Processed chunk 41/202\n",
      "Processed chunk 42/202\n",
      "Processed chunk 43/202\n",
      "Processed chunk 44/202\n",
      "Processed chunk 45/202\n",
      "Processed chunk 46/202\n",
      "Processed chunk 47/202\n",
      "Processed chunk 48/202\n",
      "Processed chunk 49/202\n",
      "Processed chunk 50/202\n",
      "Processed chunk 51/202\n",
      "Processed chunk 52/202\n",
      "Processed chunk 53/202\n",
      "Processed chunk 54/202\n",
      "Processed chunk 55/202\n",
      "Processed chunk 56/202\n",
      "Processed chunk 57/202\n",
      "Processed chunk 58/202\n",
      "Processed chunk 59/202\n",
      "Processed chunk 60/202\n",
      "Processed chunk 61/202\n",
      "Processed chunk 62/202\n",
      "Processed chunk 63/202\n",
      "Processed chunk 64/202\n",
      "Processed chunk 65/202\n",
      "Processed chunk 66/202\n",
      "Processed chunk 67/202\n",
      "Processed chunk 68/202\n",
      "Processed chunk 69/202\n",
      "Processed chunk 70/202\n",
      "Processed chunk 71/202\n",
      "Processed chunk 72/202\n",
      "Processed chunk 73/202\n",
      "Processed chunk 74/202\n",
      "Processed chunk 75/202\n",
      "Processed chunk 76/202\n",
      "Processed chunk 77/202\n",
      "Processed chunk 78/202\n",
      "Processed chunk 79/202\n",
      "Processed chunk 80/202\n",
      "Processed chunk 81/202\n",
      "Processed chunk 82/202\n",
      "Processed chunk 83/202\n",
      "Processed chunk 84/202\n",
      "Processed chunk 85/202\n",
      "Processed chunk 86/202\n",
      "Processed chunk 87/202\n",
      "Processed chunk 88/202\n",
      "Processed chunk 89/202\n",
      "Processed chunk 90/202\n",
      "Processed chunk 91/202\n",
      "Processed chunk 92/202\n",
      "Processed chunk 93/202\n",
      "Processed chunk 94/202\n",
      "Processed chunk 95/202\n",
      "Processed chunk 96/202\n",
      "Processed chunk 97/202\n",
      "Processed chunk 98/202\n",
      "Processed chunk 99/202\n",
      "Processed chunk 100/202\n",
      "Processed chunk 101/202\n",
      "Processed chunk 102/202\n",
      "Processed chunk 103/202\n",
      "Processed chunk 104/202\n",
      "Processed chunk 105/202\n",
      "Processed chunk 106/202\n",
      "Processed chunk 107/202\n",
      "Processed chunk 108/202\n",
      "Processed chunk 109/202\n",
      "Processed chunk 110/202\n",
      "Processed chunk 111/202\n",
      "Processed chunk 112/202\n",
      "Processed chunk 113/202\n",
      "Processed chunk 114/202\n",
      "Processed chunk 115/202\n",
      "Processed chunk 116/202\n",
      "Processed chunk 117/202\n",
      "Processed chunk 118/202\n",
      "Processed chunk 119/202\n",
      "Processed chunk 120/202\n",
      "Processed chunk 121/202\n",
      "Processed chunk 122/202\n",
      "Processed chunk 123/202\n",
      "Processed chunk 124/202\n",
      "Processed chunk 125/202\n",
      "Processed chunk 126/202\n",
      "Processed chunk 127/202\n",
      "Processed chunk 128/202\n",
      "Processed chunk 129/202\n",
      "Processed chunk 130/202\n",
      "Processed chunk 131/202\n",
      "Processed chunk 132/202\n",
      "Processed chunk 133/202\n",
      "Processed chunk 134/202\n",
      "Processed chunk 135/202\n",
      "Processed chunk 136/202\n",
      "Processed chunk 137/202\n",
      "Processed chunk 138/202\n",
      "Processed chunk 139/202\n",
      "Processed chunk 140/202\n",
      "Processed chunk 141/202\n",
      "Processed chunk 142/202\n",
      "Processed chunk 143/202\n",
      "Processed chunk 144/202\n",
      "Processed chunk 145/202\n",
      "Processed chunk 146/202\n",
      "Processed chunk 147/202\n",
      "Processed chunk 148/202\n",
      "Processed chunk 149/202\n",
      "Processed chunk 150/202\n",
      "Processed chunk 151/202\n",
      "Processed chunk 152/202\n",
      "Processed chunk 153/202\n",
      "Processed chunk 154/202\n",
      "Processed chunk 155/202\n",
      "Processed chunk 156/202\n",
      "Processed chunk 157/202\n",
      "Processed chunk 158/202\n",
      "Processed chunk 159/202\n",
      "Processed chunk 160/202\n",
      "Processed chunk 161/202\n",
      "Processed chunk 162/202\n",
      "Processed chunk 163/202\n",
      "Processed chunk 164/202\n",
      "Processed chunk 165/202\n",
      "Processed chunk 166/202\n",
      "Processed chunk 167/202\n",
      "Processed chunk 168/202\n",
      "Processed chunk 169/202\n",
      "Processed chunk 170/202\n",
      "Processed chunk 171/202\n",
      "Processed chunk 172/202\n",
      "Processed chunk 173/202\n",
      "Processed chunk 174/202\n",
      "Processed chunk 175/202\n",
      "Processed chunk 176/202\n",
      "Processed chunk 177/202\n",
      "Processed chunk 178/202\n",
      "Processed chunk 179/202\n",
      "Processed chunk 180/202\n",
      "Processed chunk 181/202\n",
      "Processed chunk 182/202\n",
      "Processed chunk 183/202\n",
      "Processed chunk 184/202\n",
      "Processed chunk 185/202\n",
      "Processed chunk 186/202\n",
      "Processed chunk 187/202\n",
      "Processed chunk 188/202\n",
      "Processed chunk 189/202\n",
      "Processed chunk 190/202\n",
      "Processed chunk 191/202\n",
      "Processed chunk 192/202\n",
      "Processed chunk 193/202\n",
      "Processed chunk 194/202\n",
      "Processed chunk 195/202\n",
      "Processed chunk 196/202\n",
      "Processed chunk 197/202\n",
      "Processed chunk 198/202\n",
      "Processed chunk 199/202\n",
      "Processed chunk 200/202\n",
      "Processed chunk 201/202\n",
      "Processed chunk 202/202\n",
      "  Pixels with zero-filled data: 3161\n",
      "\n",
      "=== Interpolating NDBSI ===\n",
      "Processing large dataset with 10,051,518 pixels in chunks...\n",
      "Processed chunk 1/202\n",
      "Processed chunk 2/202\n",
      "Processed chunk 3/202\n",
      "Processed chunk 4/202\n",
      "Processed chunk 5/202\n",
      "Processed chunk 6/202\n",
      "Processed chunk 7/202\n",
      "Processed chunk 8/202\n",
      "Processed chunk 9/202\n",
      "Processed chunk 10/202\n",
      "Processed chunk 11/202\n",
      "Processed chunk 12/202\n",
      "Processed chunk 13/202\n",
      "Processed chunk 14/202\n",
      "Processed chunk 15/202\n",
      "Processed chunk 16/202\n",
      "Processed chunk 17/202\n",
      "Processed chunk 18/202\n",
      "Processed chunk 19/202\n",
      "Processed chunk 20/202\n",
      "Processed chunk 21/202\n",
      "Processed chunk 22/202\n",
      "Processed chunk 23/202\n",
      "Processed chunk 24/202\n",
      "Processed chunk 25/202\n",
      "Processed chunk 26/202\n",
      "Processed chunk 27/202\n",
      "Processed chunk 28/202\n",
      "Processed chunk 29/202\n",
      "Processed chunk 30/202\n",
      "Processed chunk 31/202\n",
      "Processed chunk 32/202\n",
      "Processed chunk 33/202\n",
      "Processed chunk 34/202\n",
      "Processed chunk 35/202\n",
      "Processed chunk 36/202\n",
      "Processed chunk 37/202\n",
      "Processed chunk 38/202\n",
      "Processed chunk 39/202\n",
      "Processed chunk 40/202\n",
      "Processed chunk 41/202\n",
      "Processed chunk 42/202\n",
      "Processed chunk 43/202\n",
      "Processed chunk 44/202\n",
      "Processed chunk 45/202\n",
      "Processed chunk 46/202\n",
      "Processed chunk 47/202\n",
      "Processed chunk 48/202\n",
      "Processed chunk 49/202\n",
      "Processed chunk 50/202\n",
      "Processed chunk 51/202\n",
      "Processed chunk 52/202\n",
      "Processed chunk 53/202\n",
      "Processed chunk 54/202\n",
      "Processed chunk 55/202\n",
      "Processed chunk 56/202\n",
      "Processed chunk 57/202\n",
      "Processed chunk 58/202\n",
      "Processed chunk 59/202\n",
      "Processed chunk 60/202\n",
      "Processed chunk 61/202\n",
      "Processed chunk 62/202\n",
      "Processed chunk 63/202\n",
      "Processed chunk 64/202\n",
      "Processed chunk 65/202\n",
      "Processed chunk 66/202\n",
      "Processed chunk 67/202\n",
      "Processed chunk 68/202\n",
      "Processed chunk 69/202\n",
      "Processed chunk 70/202\n",
      "Processed chunk 71/202\n",
      "Processed chunk 72/202\n",
      "Processed chunk 73/202\n",
      "Processed chunk 74/202\n",
      "Processed chunk 75/202\n",
      "Processed chunk 76/202\n",
      "Processed chunk 77/202\n",
      "Processed chunk 78/202\n",
      "Processed chunk 79/202\n",
      "Processed chunk 80/202\n",
      "Processed chunk 81/202\n",
      "Processed chunk 82/202\n",
      "Processed chunk 83/202\n",
      "Processed chunk 84/202\n",
      "Processed chunk 85/202\n",
      "Processed chunk 86/202\n",
      "Processed chunk 87/202\n",
      "Processed chunk 88/202\n",
      "Processed chunk 89/202\n",
      "Processed chunk 90/202\n",
      "Processed chunk 91/202\n",
      "Processed chunk 92/202\n",
      "Processed chunk 93/202\n",
      "Processed chunk 94/202\n",
      "Processed chunk 95/202\n",
      "Processed chunk 96/202\n",
      "Processed chunk 97/202\n",
      "Processed chunk 98/202\n",
      "Processed chunk 99/202\n",
      "Processed chunk 100/202\n",
      "Processed chunk 101/202\n",
      "Processed chunk 102/202\n",
      "Processed chunk 103/202\n",
      "Processed chunk 104/202\n",
      "Processed chunk 105/202\n",
      "Processed chunk 106/202\n",
      "Processed chunk 107/202\n",
      "Processed chunk 108/202\n",
      "Processed chunk 109/202\n",
      "Processed chunk 110/202\n",
      "Processed chunk 111/202\n",
      "Processed chunk 112/202\n",
      "Processed chunk 113/202\n",
      "Processed chunk 114/202\n",
      "Processed chunk 115/202\n",
      "Processed chunk 116/202\n",
      "Processed chunk 117/202\n",
      "Processed chunk 118/202\n",
      "Processed chunk 119/202\n",
      "Processed chunk 120/202\n",
      "Processed chunk 121/202\n",
      "Processed chunk 122/202\n",
      "Processed chunk 123/202\n",
      "Processed chunk 124/202\n",
      "Processed chunk 125/202\n",
      "Processed chunk 126/202\n",
      "Processed chunk 127/202\n",
      "Processed chunk 128/202\n",
      "Processed chunk 129/202\n",
      "Processed chunk 130/202\n",
      "Processed chunk 131/202\n",
      "Processed chunk 132/202\n",
      "Processed chunk 133/202\n",
      "Processed chunk 134/202\n",
      "Processed chunk 135/202\n",
      "Processed chunk 136/202\n",
      "Processed chunk 137/202\n",
      "Processed chunk 138/202\n",
      "Processed chunk 139/202\n",
      "Processed chunk 140/202\n",
      "Processed chunk 141/202\n",
      "Processed chunk 142/202\n",
      "Processed chunk 143/202\n",
      "Processed chunk 144/202\n",
      "Processed chunk 145/202\n",
      "Processed chunk 146/202\n",
      "Processed chunk 147/202\n",
      "Processed chunk 148/202\n",
      "Processed chunk 149/202\n",
      "Processed chunk 150/202\n",
      "Processed chunk 151/202\n",
      "Processed chunk 152/202\n",
      "Processed chunk 153/202\n",
      "Processed chunk 154/202\n",
      "Processed chunk 155/202\n",
      "Processed chunk 156/202\n",
      "Processed chunk 157/202\n",
      "Processed chunk 158/202\n",
      "Processed chunk 159/202\n",
      "Processed chunk 160/202\n",
      "Processed chunk 161/202\n",
      "Processed chunk 162/202\n",
      "Processed chunk 163/202\n",
      "Processed chunk 164/202\n",
      "Processed chunk 165/202\n",
      "Processed chunk 166/202\n",
      "Processed chunk 167/202\n",
      "Processed chunk 168/202\n",
      "Processed chunk 169/202\n",
      "Processed chunk 170/202\n",
      "Processed chunk 171/202\n",
      "Processed chunk 172/202\n",
      "Processed chunk 173/202\n",
      "Processed chunk 174/202\n",
      "Processed chunk 175/202\n",
      "Processed chunk 176/202\n",
      "Processed chunk 177/202\n",
      "Processed chunk 178/202\n",
      "Processed chunk 179/202\n",
      "Processed chunk 180/202\n",
      "Processed chunk 181/202\n",
      "Processed chunk 182/202\n",
      "Processed chunk 183/202\n",
      "Processed chunk 184/202\n",
      "Processed chunk 185/202\n",
      "Processed chunk 186/202\n",
      "Processed chunk 187/202\n",
      "Processed chunk 188/202\n",
      "Processed chunk 189/202\n",
      "Processed chunk 190/202\n",
      "Processed chunk 191/202\n",
      "Processed chunk 192/202\n",
      "Processed chunk 193/202\n",
      "Processed chunk 194/202\n",
      "Processed chunk 195/202\n",
      "Processed chunk 196/202\n",
      "Processed chunk 197/202\n",
      "Processed chunk 198/202\n",
      "Processed chunk 199/202\n",
      "Processed chunk 200/202\n",
      "Processed chunk 201/202\n",
      "Processed chunk 202/202\n",
      "  Pixels with zero-filled data: 3161\n",
      "\n",
      "=== Interpolating CBI ===\n",
      "Processing large dataset with 10,051,518 pixels in chunks...\n",
      "Processed chunk 1/202\n",
      "Processed chunk 2/202\n",
      "Processed chunk 3/202\n",
      "Processed chunk 4/202\n",
      "Processed chunk 5/202\n",
      "Processed chunk 6/202\n",
      "Processed chunk 7/202\n",
      "Processed chunk 8/202\n",
      "Processed chunk 9/202\n",
      "Processed chunk 10/202\n",
      "Processed chunk 11/202\n",
      "Processed chunk 12/202\n",
      "Processed chunk 13/202\n",
      "Processed chunk 14/202\n",
      "Processed chunk 15/202\n",
      "Processed chunk 16/202\n",
      "Processed chunk 17/202\n",
      "Processed chunk 18/202\n",
      "Processed chunk 19/202\n",
      "Processed chunk 20/202\n",
      "Processed chunk 21/202\n",
      "Processed chunk 22/202\n",
      "Processed chunk 23/202\n",
      "Processed chunk 24/202\n",
      "Processed chunk 25/202\n",
      "Processed chunk 26/202\n",
      "Processed chunk 27/202\n",
      "Processed chunk 28/202\n",
      "Processed chunk 29/202\n",
      "Processed chunk 30/202\n",
      "Processed chunk 31/202\n",
      "Processed chunk 32/202\n",
      "Processed chunk 33/202\n",
      "Processed chunk 34/202\n",
      "Processed chunk 35/202\n",
      "Processed chunk 36/202\n",
      "Processed chunk 37/202\n",
      "Processed chunk 38/202\n",
      "Processed chunk 39/202\n",
      "Processed chunk 40/202\n",
      "Processed chunk 41/202\n",
      "Processed chunk 42/202\n",
      "Processed chunk 43/202\n",
      "Processed chunk 44/202\n",
      "Processed chunk 45/202\n",
      "Processed chunk 46/202\n",
      "Processed chunk 47/202\n",
      "Processed chunk 48/202\n",
      "Processed chunk 49/202\n",
      "Processed chunk 50/202\n",
      "Processed chunk 51/202\n",
      "Processed chunk 52/202\n",
      "Processed chunk 53/202\n",
      "Processed chunk 54/202\n",
      "Processed chunk 55/202\n",
      "Processed chunk 56/202\n",
      "Processed chunk 57/202\n",
      "Processed chunk 58/202\n",
      "Processed chunk 59/202\n",
      "Processed chunk 60/202\n",
      "Processed chunk 61/202\n",
      "Processed chunk 62/202\n",
      "Processed chunk 63/202\n",
      "Processed chunk 64/202\n",
      "Processed chunk 65/202\n",
      "Processed chunk 66/202\n",
      "Processed chunk 67/202\n",
      "Processed chunk 68/202\n",
      "Processed chunk 69/202\n",
      "Processed chunk 70/202\n",
      "Processed chunk 71/202\n",
      "Processed chunk 72/202\n",
      "Processed chunk 73/202\n",
      "Processed chunk 74/202\n",
      "Processed chunk 75/202\n",
      "Processed chunk 76/202\n",
      "Processed chunk 77/202\n",
      "Processed chunk 78/202\n",
      "Processed chunk 79/202\n",
      "Processed chunk 80/202\n",
      "Processed chunk 81/202\n",
      "Processed chunk 82/202\n",
      "Processed chunk 83/202\n",
      "Processed chunk 84/202\n",
      "Processed chunk 85/202\n",
      "Processed chunk 86/202\n",
      "Processed chunk 87/202\n",
      "Processed chunk 88/202\n",
      "Processed chunk 89/202\n",
      "Processed chunk 90/202\n",
      "Processed chunk 91/202\n",
      "Processed chunk 92/202\n",
      "Processed chunk 93/202\n",
      "Processed chunk 94/202\n",
      "Processed chunk 95/202\n",
      "Processed chunk 96/202\n",
      "Processed chunk 97/202\n",
      "Processed chunk 98/202\n",
      "Processed chunk 99/202\n",
      "Processed chunk 100/202\n",
      "Processed chunk 101/202\n",
      "Processed chunk 102/202\n",
      "Processed chunk 103/202\n",
      "Processed chunk 104/202\n",
      "Processed chunk 105/202\n",
      "Processed chunk 106/202\n",
      "Processed chunk 107/202\n",
      "Processed chunk 108/202\n",
      "Processed chunk 109/202\n",
      "Processed chunk 110/202\n",
      "Processed chunk 111/202\n",
      "Processed chunk 112/202\n",
      "Processed chunk 113/202\n",
      "Processed chunk 114/202\n",
      "Processed chunk 115/202\n",
      "Processed chunk 116/202\n",
      "Processed chunk 117/202\n",
      "Processed chunk 118/202\n",
      "Processed chunk 119/202\n",
      "Processed chunk 120/202\n",
      "Processed chunk 121/202\n",
      "Processed chunk 122/202\n",
      "Processed chunk 123/202\n",
      "Processed chunk 124/202\n",
      "Processed chunk 125/202\n",
      "Processed chunk 126/202\n",
      "Processed chunk 127/202\n",
      "Processed chunk 128/202\n",
      "Processed chunk 129/202\n",
      "Processed chunk 130/202\n",
      "Processed chunk 131/202\n",
      "Processed chunk 132/202\n",
      "Processed chunk 133/202\n",
      "Processed chunk 134/202\n",
      "Processed chunk 135/202\n",
      "Processed chunk 136/202\n",
      "Processed chunk 137/202\n",
      "Processed chunk 138/202\n",
      "Processed chunk 139/202\n",
      "Processed chunk 140/202\n",
      "Processed chunk 141/202\n",
      "Processed chunk 142/202\n",
      "Processed chunk 143/202\n",
      "Processed chunk 144/202\n",
      "Processed chunk 145/202\n",
      "Processed chunk 146/202\n",
      "Processed chunk 147/202\n",
      "Processed chunk 148/202\n",
      "Processed chunk 149/202\n",
      "Processed chunk 150/202\n",
      "Processed chunk 151/202\n",
      "Processed chunk 152/202\n",
      "Processed chunk 153/202\n",
      "Processed chunk 154/202\n",
      "Processed chunk 155/202\n",
      "Processed chunk 156/202\n",
      "Processed chunk 157/202\n",
      "Processed chunk 158/202\n",
      "Processed chunk 159/202\n",
      "Processed chunk 160/202\n",
      "Processed chunk 161/202\n",
      "Processed chunk 162/202\n",
      "Processed chunk 163/202\n",
      "Processed chunk 164/202\n",
      "Processed chunk 165/202\n",
      "Processed chunk 166/202\n",
      "Processed chunk 167/202\n",
      "Processed chunk 168/202\n",
      "Processed chunk 169/202\n",
      "Processed chunk 170/202\n",
      "Processed chunk 171/202\n",
      "Processed chunk 172/202\n",
      "Processed chunk 173/202\n",
      "Processed chunk 174/202\n",
      "Processed chunk 175/202\n",
      "Processed chunk 176/202\n",
      "Processed chunk 177/202\n",
      "Processed chunk 178/202\n",
      "Processed chunk 179/202\n",
      "Processed chunk 180/202\n",
      "Processed chunk 181/202\n",
      "Processed chunk 182/202\n",
      "Processed chunk 183/202\n",
      "Processed chunk 184/202\n",
      "Processed chunk 185/202\n",
      "Processed chunk 186/202\n",
      "Processed chunk 187/202\n",
      "Processed chunk 188/202\n",
      "Processed chunk 189/202\n",
      "Processed chunk 190/202\n",
      "Processed chunk 191/202\n",
      "Processed chunk 192/202\n",
      "Processed chunk 193/202\n",
      "Processed chunk 194/202\n",
      "Processed chunk 195/202\n",
      "Processed chunk 196/202\n",
      "Processed chunk 197/202\n",
      "Processed chunk 198/202\n",
      "Processed chunk 199/202\n",
      "Processed chunk 200/202\n",
      "Processed chunk 201/202\n",
      "Processed chunk 202/202\n",
      "  Pixels with zero-filled data: 3161\n",
      "\n",
      "=== Interpolating UCI ===\n",
      "Processing large dataset with 10,051,518 pixels in chunks...\n",
      "Processed chunk 1/202\n",
      "Processed chunk 2/202\n",
      "Processed chunk 3/202\n",
      "Processed chunk 4/202\n",
      "Processed chunk 5/202\n",
      "Processed chunk 6/202\n",
      "Processed chunk 7/202\n",
      "Processed chunk 8/202\n",
      "Processed chunk 9/202\n",
      "Processed chunk 10/202\n",
      "Processed chunk 11/202\n",
      "Processed chunk 12/202\n",
      "Processed chunk 13/202\n",
      "Processed chunk 14/202\n",
      "Processed chunk 15/202\n",
      "Processed chunk 16/202\n",
      "Processed chunk 17/202\n",
      "Processed chunk 18/202\n",
      "Processed chunk 19/202\n",
      "Processed chunk 20/202\n",
      "Processed chunk 21/202\n",
      "Processed chunk 22/202\n",
      "Processed chunk 23/202\n",
      "Processed chunk 24/202\n",
      "Processed chunk 25/202\n",
      "Processed chunk 26/202\n",
      "Processed chunk 27/202\n",
      "Processed chunk 28/202\n",
      "Processed chunk 29/202\n",
      "Processed chunk 30/202\n",
      "Processed chunk 31/202\n",
      "Processed chunk 32/202\n",
      "Processed chunk 33/202\n",
      "Processed chunk 34/202\n",
      "Processed chunk 35/202\n",
      "Processed chunk 36/202\n",
      "Processed chunk 37/202\n",
      "Processed chunk 38/202\n",
      "Processed chunk 39/202\n",
      "Processed chunk 40/202\n",
      "Processed chunk 41/202\n",
      "Processed chunk 42/202\n",
      "Processed chunk 43/202\n",
      "Processed chunk 44/202\n",
      "Processed chunk 45/202\n",
      "Processed chunk 46/202\n",
      "Processed chunk 47/202\n",
      "Processed chunk 48/202\n",
      "Processed chunk 49/202\n",
      "Processed chunk 50/202\n",
      "Processed chunk 51/202\n",
      "Processed chunk 52/202\n",
      "Processed chunk 53/202\n",
      "Processed chunk 54/202\n",
      "Processed chunk 55/202\n",
      "Processed chunk 56/202\n",
      "Processed chunk 57/202\n",
      "Processed chunk 58/202\n",
      "Processed chunk 59/202\n",
      "Processed chunk 60/202\n",
      "Processed chunk 61/202\n",
      "Processed chunk 62/202\n",
      "Processed chunk 63/202\n",
      "Processed chunk 64/202\n",
      "Processed chunk 65/202\n",
      "Processed chunk 66/202\n",
      "Processed chunk 67/202\n",
      "Processed chunk 68/202\n",
      "Processed chunk 69/202\n",
      "Processed chunk 70/202\n",
      "Processed chunk 71/202\n",
      "Processed chunk 72/202\n",
      "Processed chunk 73/202\n",
      "Processed chunk 74/202\n",
      "Processed chunk 75/202\n",
      "Processed chunk 76/202\n",
      "Processed chunk 77/202\n",
      "Processed chunk 78/202\n",
      "Processed chunk 79/202\n",
      "Processed chunk 80/202\n",
      "Processed chunk 81/202\n",
      "Processed chunk 82/202\n",
      "Processed chunk 83/202\n",
      "Processed chunk 84/202\n",
      "Processed chunk 85/202\n",
      "Processed chunk 86/202\n",
      "Processed chunk 87/202\n",
      "Processed chunk 88/202\n",
      "Processed chunk 89/202\n",
      "Processed chunk 90/202\n",
      "Processed chunk 91/202\n",
      "Processed chunk 92/202\n",
      "Processed chunk 93/202\n",
      "Processed chunk 94/202\n",
      "Processed chunk 95/202\n",
      "Processed chunk 96/202\n",
      "Processed chunk 97/202\n",
      "Processed chunk 98/202\n",
      "Processed chunk 99/202\n",
      "Processed chunk 100/202\n",
      "Processed chunk 101/202\n",
      "Processed chunk 102/202\n",
      "Processed chunk 103/202\n",
      "Processed chunk 104/202\n",
      "Processed chunk 105/202\n",
      "Processed chunk 106/202\n",
      "Processed chunk 107/202\n",
      "Processed chunk 108/202\n",
      "Processed chunk 109/202\n",
      "Processed chunk 110/202\n",
      "Processed chunk 111/202\n",
      "Processed chunk 112/202\n",
      "Processed chunk 113/202\n",
      "Processed chunk 114/202\n",
      "Processed chunk 115/202\n",
      "Processed chunk 116/202\n",
      "Processed chunk 117/202\n",
      "Processed chunk 118/202\n",
      "Processed chunk 119/202\n",
      "Processed chunk 120/202\n",
      "Processed chunk 121/202\n",
      "Processed chunk 122/202\n",
      "Processed chunk 123/202\n",
      "Processed chunk 124/202\n",
      "Processed chunk 125/202\n",
      "Processed chunk 126/202\n",
      "Processed chunk 127/202\n",
      "Processed chunk 128/202\n",
      "Processed chunk 129/202\n",
      "Processed chunk 130/202\n",
      "Processed chunk 131/202\n",
      "Processed chunk 132/202\n",
      "Processed chunk 133/202\n",
      "Processed chunk 134/202\n",
      "Processed chunk 135/202\n",
      "Processed chunk 136/202\n",
      "Processed chunk 137/202\n",
      "Processed chunk 138/202\n",
      "Processed chunk 139/202\n",
      "Processed chunk 140/202\n",
      "Processed chunk 141/202\n",
      "Processed chunk 142/202\n",
      "Processed chunk 143/202\n",
      "Processed chunk 144/202\n",
      "Processed chunk 145/202\n",
      "Processed chunk 146/202\n",
      "Processed chunk 147/202\n",
      "Processed chunk 148/202\n",
      "Processed chunk 149/202\n",
      "Processed chunk 150/202\n",
      "Processed chunk 151/202\n",
      "Processed chunk 152/202\n",
      "Processed chunk 153/202\n",
      "Processed chunk 154/202\n",
      "Processed chunk 155/202\n",
      "Processed chunk 156/202\n",
      "Processed chunk 157/202\n",
      "Processed chunk 158/202\n",
      "Processed chunk 159/202\n",
      "Processed chunk 160/202\n",
      "Processed chunk 161/202\n",
      "Processed chunk 162/202\n",
      "Processed chunk 163/202\n",
      "Processed chunk 164/202\n",
      "Processed chunk 165/202\n",
      "Processed chunk 166/202\n",
      "Processed chunk 167/202\n",
      "Processed chunk 168/202\n",
      "Processed chunk 169/202\n",
      "Processed chunk 170/202\n",
      "Processed chunk 171/202\n",
      "Processed chunk 172/202\n",
      "Processed chunk 173/202\n",
      "Processed chunk 174/202\n",
      "Processed chunk 175/202\n",
      "Processed chunk 176/202\n",
      "Processed chunk 177/202\n",
      "Processed chunk 178/202\n",
      "Processed chunk 179/202\n",
      "Processed chunk 180/202\n",
      "Processed chunk 181/202\n",
      "Processed chunk 182/202\n",
      "Processed chunk 183/202\n",
      "Processed chunk 184/202\n",
      "Processed chunk 185/202\n",
      "Processed chunk 186/202\n",
      "Processed chunk 187/202\n",
      "Processed chunk 188/202\n",
      "Processed chunk 189/202\n",
      "Processed chunk 190/202\n",
      "Processed chunk 191/202\n",
      "Processed chunk 192/202\n",
      "Processed chunk 193/202\n",
      "Processed chunk 194/202\n",
      "Processed chunk 195/202\n",
      "Processed chunk 196/202\n",
      "Processed chunk 197/202\n",
      "Processed chunk 198/202\n",
      "Processed chunk 199/202\n",
      "Processed chunk 200/202\n",
      "Processed chunk 201/202\n",
      "Processed chunk 202/202\n",
      "  Pixels with zero-filled data: 3161\n",
      "CPU times: user 46min 49s, sys: 6.66 s, total: 46min 56s\n",
      "Wall time: 46min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"\\n=== Interpolating NDVI ===\")\n",
    "img_ndvi_int, mask_ndvi = clean_and_interpolate_with_mask(img_ndvi_reshape)\n",
    "\n",
    "print(\"\\n=== Interpolating MNDWI ===\")\n",
    "img_mndwi_int, mask_mndwi = clean_and_interpolate_with_mask(img_mndwi_reshape)\n",
    "\n",
    "print(\"\\n=== Interpolating NDBI ===\")\n",
    "img_ndbi_int, mask_ndbi = clean_and_interpolate_with_mask(img_ndbi_reshape)\n",
    "\n",
    "print(\"\\n=== Interpolating NDBSI ===\")\n",
    "img_ndbsi_int, mask_ndbsi = clean_and_interpolate_with_mask(img_ndbsi_reshape)\n",
    "\n",
    "print(\"\\n=== Interpolating CBI ===\")\n",
    "img_cbi_int, mask_cbi = clean_and_interpolate_with_mask(img_cbi_reshape)\n",
    "\n",
    "print(\"\\n=== Interpolating UCI ===\")\n",
    "img_uci_int, mask_uci = clean_and_interpolate_with_mask(img_uci_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9966c36-b995-45d1-a01d-ef644a2881a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data shape: (10051518, 24, 6)\n",
      "Final mask shape: (10051518, 24, 6)\n",
      "Final mask shape: (10051518, 24)\n"
     ]
    }
   ],
   "source": [
    "comb_all = np.stack([img_ndvi_int, img_mndwi_int, img_ndbi_int, \n",
    "                     img_ndbsi_int, img_cbi_int, img_uci_int], axis=-1)\n",
    "\n",
    "print(f\"Combined data shape: {comb_all.shape}\")  # Should be [n_pixels, 24, 6]\n",
    "\n",
    "# Stack the MASKS\n",
    "confidence_mask_combined = np.stack([mask_ndvi, mask_mndwi, mask_ndbi,\n",
    "                                     mask_ndbsi, mask_cbi, mask_uci], axis=-1)  # [n_pixels, 24, 6]\n",
    "\n",
    "confidence_mask_final = confidence_mask_combined.mean(axis=-1)\n",
    "print(f\"Final mask shape: {confidence_mask_combined.shape}\")\n",
    "print(f\"Final mask shape: {confidence_mask_final.shape}\") \n",
    "\n",
    "# # Clean up\n",
    "# del img_ndvi_int, img_mndwi_int, img_ndbi_int, img_ndbsi_int, img_cbi_int, img_uci_int\n",
    "# del img_ndvi_reshape, img_mndwi_reshape, img_ndbi_reshape, img_ndbsi_reshape, img_cbi_reshape, img_uci_reshape\n",
    "# del mask_ndvi, mask_mndwi, mask_ndbi, mask_ndbsi, mask_cbi, mask_uci\n",
    "# # del confidence_mask_combined\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37d9200f-e2f6-4276-bb75-91c549b99d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stack all bands\n",
    "# print(\"Stacking all bands...\")\n",
    "# comb_all = np.stack((img_ndvi_int, img_mndwi_int, \n",
    "#                      img_ndbi_int, img_ndbsi_int, img_cbi_int, img_uci_int), axis=1)\n",
    "\n",
    "# # Reshape for model input\n",
    "# comb_all_reshape = comb_all.reshape(comb_all.shape[0], comb_all.shape[2], comb_all.shape[1])\n",
    "# print(f\"Final input shape: {comb_all_reshape.shape}\")\n",
    "\n",
    "# # Clear intermediate variables to free memory\n",
    "# del img_ndvi_int, img_mndwi_int, img_ndbi_int, img_ndbsi_int, img_cbi_int, img_uci_int\n",
    "# del img_ndvi_reshape, img_mndwi_reshape, img_ndbi_reshape, img_ndbsi_reshape, img_cbi_reshape, img_uci_reshape\n",
    "# del comb_all\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce8e0805-4467-4e66-8f2d-f20f2e4a18e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comb_all = np.stack((img_ndvi_int, img_mndwi_int, \n",
    "#                      img_ndbi_int, img_ndbsi_int, \n",
    "#                      img_cbi_int, img_uci_int), axis=1)\n",
    "\n",
    "# # Reshape: [n_pixels, 6_bands, n_timesteps] → [n_pixels, n_timesteps, 6_bands]\n",
    "# comb_all_reshape = comb_all.reshape(comb_all.shape[0], comb_all.shape[2], comb_all.shape[1])\n",
    "# print(f\"Final input shape: {comb_all_reshape.shape}\")  # Should be [n_pixels, 24, 6]\n",
    "# print(f\"Confidence mask shape: {confidence_mask_combined.shape}\")  # Should be [n_pixels, 24]\n",
    "\n",
    "# # Clear intermediate variables\n",
    "# del img_ndvi_int, img_mndwi_int, img_ndbi_int, img_ndbsi_int, img_cbi_int, img_uci_int\n",
    "# del img_ndvi_reshape, img_mndwi_reshape, img_ndbi_reshape, img_ndbsi_reshape, img_cbi_reshape, img_uci_reshape\n",
    "# del mask_ndvi, mask_mndwi, mask_ndbi, mask_ndbsi, mask_cbi, mask_uci, masks_stacked\n",
    "# del comb_all\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69033f9c-ad38-4467-bb18-256ae43e5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BATCH PREDICTION - Process in smaller chunks to avoid memory issues\n",
    "# def predict_in_batches(model, data, batch_size=5000):\n",
    "#     \"\"\"Predict data in batches to manage memory\"\"\"\n",
    "#     n_samples = data.shape[0]\n",
    "#     predictions = []\n",
    "    \n",
    "#     print(f\"Predicting {n_samples:,} pixels in batches of {batch_size:,}...\")\n",
    "    \n",
    "#     for i in range(0, n_samples, batch_size):\n",
    "#         end_idx = min(i + batch_size, n_samples)\n",
    "#         batch = data[i:end_idx]\n",
    "#         batch = np.asarray(batch, dtype=np.float32)\n",
    "        \n",
    "#         try:\n",
    "#             batch_pred = model.predict(batch, verbose=0)\n",
    "#             predictions.append(batch_pred)\n",
    "#             print(f\"Processed batch {i//batch_size + 1}/{(n_samples-1)//batch_size + 1}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error in batch {i//batch_size + 1}: {e}\")\n",
    "#             # Try with smaller batch size\n",
    "#             smaller_batch_size = batch_size // 2\n",
    "#             print(f\"Retrying with smaller batch size: {smaller_batch_size}\")\n",
    "#             for j in range(i, end_idx, smaller_batch_size):\n",
    "#                 small_end = min(j + smaller_batch_size, end_idx)\n",
    "#                 small_batch = data[j:small_end]\n",
    "#                 small_batch = np.asarray(small_batch, dtype=np.float32)\n",
    "#                 small_pred = model.predict(small_batch, verbose=0)\n",
    "#                 predictions.append(small_pred)\n",
    "        \n",
    "#         tf.keras.backend.clear_session()\n",
    "#         gc.collect()\n",
    "    \n",
    "#     return np.vstack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9c80ede-8545-4c94-be01-f251997414ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_in_batches_with_mask(model, data, masks, batch_size=5000):\n",
    "    \"\"\"\n",
    "    Predict data in batches with confidence masks\n",
    "    model: Keras model expecting [data, mask] inputs\n",
    "    data: [n_pixels, n_timesteps, n_bands]\n",
    "    masks: [n_pixels, n_timesteps]\n",
    "    \"\"\"\n",
    "    n_samples = data.shape[0]\n",
    "    predictions = []\n",
    "    \n",
    "    print(f\"Predicting {n_samples:,} pixels in batches of {batch_size:,}...\")\n",
    "    \n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        end_idx = min(i + batch_size, n_samples)\n",
    "        batch_data = data[i:end_idx]\n",
    "        batch_mask = masks[i:end_idx]\n",
    "        \n",
    "        # Ensure correct dtypes\n",
    "        batch_data = np.asarray(batch_data, dtype=np.float32)\n",
    "        batch_mask = np.asarray(batch_mask, dtype=np.float32)\n",
    "        \n",
    "        try:\n",
    "            # CRITICAL: Pass BOTH inputs as a list\n",
    "            batch_pred = model.predict([batch_data, batch_mask], verbose=0)\n",
    "            predictions.append(batch_pred)\n",
    "            \n",
    "            if (i // batch_size + 1) % 10 == 0:  # Print every 10 batches\n",
    "                print(f\"Processed batch {i//batch_size + 1}/{(n_samples-1)//batch_size + 1}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i//batch_size + 1}: {e}\")\n",
    "            # Try with smaller batch size\n",
    "            smaller_batch_size = batch_size // 2\n",
    "            print(f\"Retrying with smaller batch size: {smaller_batch_size}\")\n",
    "            \n",
    "            for j in range(i, end_idx, smaller_batch_size):\n",
    "                small_end = min(j + smaller_batch_size, end_idx)\n",
    "                small_batch_data = data[j:small_end]\n",
    "                small_batch_mask = masks[j:small_end]\n",
    "                \n",
    "                small_batch_data = np.asarray(small_batch_data, dtype=np.float32)\n",
    "                small_batch_mask = np.asarray(small_batch_mask, dtype=np.float32)\n",
    "                \n",
    "                small_pred = model.predict([small_batch_data, small_batch_mask], verbose=0)\n",
    "                predictions.append(small_pred)\n",
    "        \n",
    "        # Clear session periodically to avoid memory buildup\n",
    "        if (i // batch_size + 1) % 50 == 0:\n",
    "            tf.keras.backend.clear_session()\n",
    "            gc.collect()\n",
    "    \n",
    "    print(\"Combining predictions...\")\n",
    "    return np.vstack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "05162610-941d-4749-b4e7-0819ed478ce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Perform prediction\n",
    "# print(\"Starting prediction...\")\n",
    "# try:\n",
    "#     pred_ = predict_in_batches(model, comb_all_reshape, batch_size=2000)\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"Error during prediction: {e}\")\n",
    "#     print(\"Trying with even smaller batch size...\")\n",
    "#     try:\n",
    "#         pred_ = predict_in_batches(model, comb_all_reshape, batch_size=1000)\n",
    "#     except Exception as e2:\n",
    "#         print(f\"Still getting error: {e2}\")\n",
    "#         print(\"Trying CPU-only prediction...\")\n",
    "#         with tf.device('/CPU:0'):\n",
    "#             pred_ = predict_in_batches(model, comb_all_reshape, batch_size=1000)\n",
    "\n",
    "# print(\"Processing predictions...\")\n",
    "# lc_pred_ = pred_.argmax(axis=1)\n",
    "\n",
    "# # Reshape back to image dimensions\n",
    "# img_pred = np.reshape(lc_pred_, (ds_ndvi.RasterYSize, ds_ndvi.RasterXSize))\n",
    "\n",
    "# print(f\"Prediction shape: {img_pred.shape}\")\n",
    "# print(f\"Prediction min: {img_pred.min()}, max: {img_pred.max()}\")\n",
    "# print(f\"Unique classes: {np.unique(img_pred)}\")\n",
    "\n",
    "# # Export result\n",
    "# outFile = r'/home/jupyter-bryan/ISA_Data/Raster_02/ISA_MND_Multi_Orig_MCTNet.tif'\n",
    "# raster.export(img_pred, ds_ndvi, filename=outFile, dtype='int')\n",
    "# print(f\"Prediction saved to: {outFile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4dbc4cf1-76d8-4c03-b52a-23ae7b725904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting prediction with ALPE model ===\n",
      "Predicting 10,051,518 pixels in batches of 2,000...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-19 17:34:15.870718: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 10/5026\n",
      "Processed batch 20/5026\n",
      "Processed batch 30/5026\n",
      "Processed batch 40/5026\n",
      "Processed batch 50/5026\n",
      "Processed batch 60/5026\n",
      "Processed batch 70/5026\n",
      "Processed batch 80/5026\n",
      "Processed batch 90/5026\n",
      "Processed batch 100/5026\n",
      "Processed batch 110/5026\n",
      "Processed batch 120/5026\n",
      "Processed batch 130/5026\n",
      "Processed batch 140/5026\n",
      "Processed batch 150/5026\n",
      "Processed batch 160/5026\n",
      "Processed batch 170/5026\n",
      "Processed batch 180/5026\n",
      "Processed batch 190/5026\n",
      "Processed batch 200/5026\n",
      "Processed batch 210/5026\n",
      "Processed batch 220/5026\n",
      "Processed batch 230/5026\n",
      "Processed batch 240/5026\n",
      "Processed batch 250/5026\n",
      "Processed batch 260/5026\n",
      "Processed batch 270/5026\n",
      "Processed batch 280/5026\n",
      "Processed batch 290/5026\n",
      "Processed batch 300/5026\n",
      "Processed batch 310/5026\n",
      "Processed batch 320/5026\n",
      "Processed batch 330/5026\n",
      "Processed batch 340/5026\n",
      "Processed batch 350/5026\n",
      "Processed batch 360/5026\n",
      "Processed batch 370/5026\n",
      "Processed batch 380/5026\n",
      "Processed batch 390/5026\n",
      "Processed batch 400/5026\n",
      "Processed batch 410/5026\n",
      "Processed batch 420/5026\n",
      "Processed batch 430/5026\n",
      "Processed batch 440/5026\n",
      "Processed batch 450/5026\n",
      "Processed batch 460/5026\n",
      "Processed batch 470/5026\n",
      "Processed batch 480/5026\n",
      "Processed batch 490/5026\n",
      "Processed batch 500/5026\n",
      "Processed batch 510/5026\n",
      "Processed batch 520/5026\n",
      "Processed batch 530/5026\n",
      "Processed batch 540/5026\n",
      "Processed batch 550/5026\n",
      "Processed batch 560/5026\n",
      "Processed batch 570/5026\n",
      "Processed batch 580/5026\n",
      "Processed batch 590/5026\n",
      "Processed batch 600/5026\n",
      "Processed batch 610/5026\n",
      "Processed batch 620/5026\n",
      "Processed batch 630/5026\n",
      "Processed batch 640/5026\n",
      "Processed batch 650/5026\n",
      "Processed batch 660/5026\n",
      "Processed batch 670/5026\n",
      "Processed batch 680/5026\n",
      "Processed batch 690/5026\n",
      "Processed batch 700/5026\n",
      "Processed batch 710/5026\n",
      "Processed batch 720/5026\n",
      "Processed batch 730/5026\n",
      "Processed batch 740/5026\n",
      "Processed batch 750/5026\n",
      "Processed batch 760/5026\n",
      "Processed batch 770/5026\n",
      "Processed batch 780/5026\n",
      "Processed batch 790/5026\n",
      "Processed batch 800/5026\n",
      "Processed batch 810/5026\n",
      "Processed batch 820/5026\n",
      "Processed batch 830/5026\n",
      "Processed batch 840/5026\n",
      "Processed batch 850/5026\n",
      "Processed batch 860/5026\n",
      "Processed batch 870/5026\n",
      "Processed batch 880/5026\n",
      "Processed batch 890/5026\n",
      "Processed batch 900/5026\n",
      "Processed batch 910/5026\n",
      "Processed batch 920/5026\n",
      "Processed batch 930/5026\n",
      "Processed batch 940/5026\n",
      "Processed batch 950/5026\n",
      "Processed batch 960/5026\n",
      "Processed batch 970/5026\n",
      "Processed batch 980/5026\n",
      "Processed batch 990/5026\n",
      "Processed batch 1000/5026\n",
      "Processed batch 1010/5026\n",
      "Processed batch 1020/5026\n",
      "Processed batch 1030/5026\n",
      "Processed batch 1040/5026\n",
      "Processed batch 1050/5026\n",
      "Processed batch 1060/5026\n",
      "Processed batch 1070/5026\n",
      "Processed batch 1080/5026\n",
      "Processed batch 1090/5026\n",
      "Processed batch 1100/5026\n",
      "Processed batch 1110/5026\n",
      "Processed batch 1120/5026\n",
      "Processed batch 1130/5026\n",
      "Processed batch 1140/5026\n",
      "Processed batch 1150/5026\n",
      "Processed batch 1160/5026\n",
      "Processed batch 1170/5026\n",
      "Processed batch 1180/5026\n",
      "Processed batch 1190/5026\n",
      "Processed batch 1200/5026\n",
      "Processed batch 1210/5026\n",
      "Processed batch 1220/5026\n",
      "Processed batch 1230/5026\n",
      "Processed batch 1240/5026\n",
      "Processed batch 1250/5026\n",
      "Processed batch 1260/5026\n",
      "Processed batch 1270/5026\n",
      "Processed batch 1280/5026\n",
      "Processed batch 1290/5026\n",
      "Processed batch 1300/5026\n",
      "Processed batch 1310/5026\n",
      "Processed batch 1320/5026\n",
      "Processed batch 1330/5026\n",
      "Processed batch 1340/5026\n",
      "Processed batch 1350/5026\n",
      "Processed batch 1360/5026\n",
      "Processed batch 1370/5026\n",
      "Processed batch 1380/5026\n",
      "Processed batch 1390/5026\n",
      "Processed batch 1400/5026\n",
      "Processed batch 1410/5026\n",
      "Processed batch 1420/5026\n",
      "Processed batch 1430/5026\n",
      "Processed batch 1440/5026\n",
      "Processed batch 1450/5026\n",
      "Processed batch 1460/5026\n",
      "Processed batch 1470/5026\n",
      "Processed batch 1480/5026\n",
      "Processed batch 1490/5026\n",
      "Processed batch 1500/5026\n",
      "Processed batch 1510/5026\n",
      "Processed batch 1520/5026\n",
      "Processed batch 1530/5026\n",
      "Processed batch 1540/5026\n",
      "Processed batch 1550/5026\n",
      "Processed batch 1560/5026\n",
      "Processed batch 1570/5026\n",
      "Processed batch 1580/5026\n",
      "Processed batch 1590/5026\n",
      "Processed batch 1600/5026\n",
      "Processed batch 1610/5026\n",
      "Processed batch 1620/5026\n",
      "Processed batch 1630/5026\n",
      "Processed batch 1640/5026\n",
      "Processed batch 1650/5026\n",
      "Processed batch 1660/5026\n",
      "Processed batch 1670/5026\n",
      "Processed batch 1680/5026\n",
      "Processed batch 1690/5026\n",
      "Processed batch 1700/5026\n",
      "Processed batch 1710/5026\n",
      "Processed batch 1720/5026\n",
      "Processed batch 1730/5026\n",
      "Processed batch 1740/5026\n",
      "Processed batch 1750/5026\n",
      "Processed batch 1760/5026\n",
      "Processed batch 1770/5026\n",
      "Processed batch 1780/5026\n",
      "Processed batch 1790/5026\n",
      "Processed batch 1800/5026\n",
      "Processed batch 1810/5026\n",
      "Processed batch 1820/5026\n",
      "Processed batch 1830/5026\n",
      "Processed batch 1840/5026\n",
      "Processed batch 1850/5026\n",
      "Processed batch 1860/5026\n",
      "Processed batch 1870/5026\n",
      "Processed batch 1880/5026\n",
      "Processed batch 1890/5026\n",
      "Processed batch 1900/5026\n",
      "Processed batch 1910/5026\n",
      "Processed batch 1920/5026\n",
      "Processed batch 1930/5026\n",
      "Processed batch 1940/5026\n",
      "Processed batch 1950/5026\n",
      "Processed batch 1960/5026\n",
      "Processed batch 1970/5026\n",
      "Processed batch 1980/5026\n",
      "Processed batch 1990/5026\n",
      "Processed batch 2000/5026\n",
      "Processed batch 2010/5026\n",
      "Processed batch 2020/5026\n",
      "Processed batch 2030/5026\n",
      "Processed batch 2040/5026\n",
      "Processed batch 2050/5026\n",
      "Processed batch 2060/5026\n",
      "Processed batch 2070/5026\n",
      "Processed batch 2080/5026\n",
      "Processed batch 2090/5026\n",
      "Processed batch 2100/5026\n",
      "Processed batch 2110/5026\n",
      "Processed batch 2120/5026\n",
      "Processed batch 2130/5026\n",
      "Processed batch 2140/5026\n",
      "Processed batch 2150/5026\n",
      "Processed batch 2160/5026\n",
      "Processed batch 2170/5026\n",
      "Processed batch 2180/5026\n",
      "Processed batch 2190/5026\n",
      "Processed batch 2200/5026\n",
      "Processed batch 2210/5026\n",
      "Processed batch 2220/5026\n",
      "Processed batch 2230/5026\n",
      "Processed batch 2240/5026\n",
      "Processed batch 2250/5026\n",
      "Processed batch 2260/5026\n",
      "Processed batch 2270/5026\n",
      "Processed batch 2280/5026\n",
      "Processed batch 2290/5026\n",
      "Processed batch 2300/5026\n",
      "Processed batch 2310/5026\n",
      "Processed batch 2320/5026\n",
      "Processed batch 2330/5026\n",
      "Processed batch 2340/5026\n",
      "Processed batch 2350/5026\n",
      "Processed batch 2360/5026\n",
      "Processed batch 2370/5026\n",
      "Processed batch 2380/5026\n",
      "Processed batch 2390/5026\n",
      "Processed batch 2400/5026\n",
      "Processed batch 2410/5026\n",
      "Processed batch 2420/5026\n",
      "Processed batch 2430/5026\n",
      "Processed batch 2440/5026\n",
      "Processed batch 2450/5026\n",
      "Processed batch 2460/5026\n",
      "Processed batch 2470/5026\n",
      "Processed batch 2480/5026\n",
      "Processed batch 2490/5026\n",
      "Processed batch 2500/5026\n",
      "Processed batch 2510/5026\n",
      "Processed batch 2520/5026\n",
      "Processed batch 2530/5026\n",
      "Processed batch 2540/5026\n",
      "Processed batch 2550/5026\n",
      "Processed batch 2560/5026\n",
      "Processed batch 2570/5026\n",
      "Processed batch 2580/5026\n",
      "Processed batch 2590/5026\n",
      "Processed batch 2600/5026\n",
      "Processed batch 2610/5026\n",
      "Processed batch 2620/5026\n",
      "Processed batch 2630/5026\n",
      "Processed batch 2640/5026\n",
      "Processed batch 2650/5026\n",
      "Processed batch 2660/5026\n",
      "Processed batch 2670/5026\n",
      "Processed batch 2680/5026\n",
      "Processed batch 2690/5026\n",
      "Processed batch 2700/5026\n",
      "Processed batch 2710/5026\n",
      "Processed batch 2720/5026\n",
      "Processed batch 2730/5026\n",
      "Processed batch 2740/5026\n",
      "Processed batch 2750/5026\n",
      "Processed batch 2760/5026\n",
      "Processed batch 2770/5026\n",
      "Processed batch 2780/5026\n",
      "Processed batch 2790/5026\n",
      "Processed batch 2800/5026\n",
      "Processed batch 2810/5026\n",
      "Processed batch 2820/5026\n",
      "Processed batch 2830/5026\n",
      "Processed batch 2840/5026\n",
      "Processed batch 2850/5026\n",
      "Processed batch 2860/5026\n",
      "Processed batch 2870/5026\n",
      "Processed batch 2880/5026\n",
      "Processed batch 2890/5026\n",
      "Processed batch 2900/5026\n",
      "Processed batch 2910/5026\n",
      "Processed batch 2920/5026\n",
      "Processed batch 2930/5026\n",
      "Processed batch 2940/5026\n",
      "Processed batch 2950/5026\n",
      "Processed batch 2960/5026\n",
      "Processed batch 2970/5026\n",
      "Processed batch 2980/5026\n",
      "Processed batch 2990/5026\n",
      "Processed batch 3000/5026\n",
      "Processed batch 3010/5026\n",
      "Processed batch 3020/5026\n",
      "Processed batch 3030/5026\n",
      "Processed batch 3040/5026\n",
      "Processed batch 3050/5026\n",
      "Processed batch 3060/5026\n",
      "Processed batch 3070/5026\n",
      "Processed batch 3080/5026\n",
      "Processed batch 3090/5026\n",
      "Processed batch 3100/5026\n",
      "Processed batch 3110/5026\n",
      "Processed batch 3120/5026\n",
      "Processed batch 3130/5026\n",
      "Processed batch 3140/5026\n",
      "Processed batch 3150/5026\n",
      "Processed batch 3160/5026\n",
      "Processed batch 3170/5026\n",
      "Processed batch 3180/5026\n",
      "Processed batch 3190/5026\n",
      "Processed batch 3200/5026\n",
      "Processed batch 3210/5026\n",
      "Processed batch 3220/5026\n",
      "Processed batch 3230/5026\n",
      "Processed batch 3240/5026\n",
      "Processed batch 3250/5026\n",
      "Processed batch 3260/5026\n",
      "Processed batch 3270/5026\n",
      "Processed batch 3280/5026\n",
      "Processed batch 3290/5026\n",
      "Processed batch 3300/5026\n",
      "Processed batch 3310/5026\n",
      "Processed batch 3320/5026\n",
      "Processed batch 3330/5026\n",
      "Processed batch 3340/5026\n",
      "Processed batch 3350/5026\n",
      "Processed batch 3360/5026\n",
      "Processed batch 3370/5026\n",
      "Processed batch 3380/5026\n",
      "Processed batch 3390/5026\n",
      "Processed batch 3400/5026\n",
      "Processed batch 3410/5026\n",
      "Processed batch 3420/5026\n",
      "Processed batch 3430/5026\n",
      "Processed batch 3440/5026\n",
      "Processed batch 3450/5026\n",
      "Processed batch 3460/5026\n",
      "Processed batch 3470/5026\n",
      "Processed batch 3480/5026\n",
      "Processed batch 3490/5026\n",
      "Processed batch 3500/5026\n",
      "Processed batch 3510/5026\n",
      "Processed batch 3520/5026\n",
      "Processed batch 3530/5026\n",
      "Processed batch 3540/5026\n",
      "Processed batch 3550/5026\n",
      "Processed batch 3560/5026\n",
      "Processed batch 3570/5026\n",
      "Processed batch 3580/5026\n",
      "Processed batch 3590/5026\n",
      "Processed batch 3600/5026\n",
      "Processed batch 3610/5026\n",
      "Processed batch 3620/5026\n",
      "Processed batch 3630/5026\n",
      "Processed batch 3640/5026\n",
      "Processed batch 3650/5026\n",
      "Processed batch 3660/5026\n",
      "Processed batch 3670/5026\n",
      "Processed batch 3680/5026\n",
      "Processed batch 3690/5026\n",
      "Processed batch 3700/5026\n",
      "Processed batch 3710/5026\n",
      "Processed batch 3720/5026\n",
      "Processed batch 3730/5026\n",
      "Processed batch 3740/5026\n",
      "Processed batch 3750/5026\n",
      "Processed batch 3760/5026\n",
      "Processed batch 3770/5026\n",
      "Processed batch 3780/5026\n",
      "Processed batch 3790/5026\n",
      "Processed batch 3800/5026\n",
      "Processed batch 3810/5026\n",
      "Processed batch 3820/5026\n",
      "Processed batch 3830/5026\n",
      "Processed batch 3840/5026\n",
      "Processed batch 3850/5026\n",
      "Processed batch 3860/5026\n",
      "Processed batch 3870/5026\n",
      "Processed batch 3880/5026\n",
      "Processed batch 3890/5026\n",
      "Processed batch 3900/5026\n",
      "Processed batch 3910/5026\n",
      "Processed batch 3920/5026\n",
      "Processed batch 3930/5026\n",
      "Processed batch 3940/5026\n",
      "Processed batch 3950/5026\n",
      "Processed batch 3960/5026\n",
      "Processed batch 3970/5026\n",
      "Processed batch 3980/5026\n",
      "Processed batch 3990/5026\n",
      "Processed batch 4000/5026\n",
      "Processed batch 4010/5026\n",
      "Processed batch 4020/5026\n",
      "Processed batch 4030/5026\n",
      "Processed batch 4040/5026\n",
      "Processed batch 4050/5026\n",
      "Processed batch 4060/5026\n",
      "Processed batch 4070/5026\n",
      "Processed batch 4080/5026\n",
      "Processed batch 4090/5026\n",
      "Processed batch 4100/5026\n",
      "Processed batch 4110/5026\n",
      "Processed batch 4120/5026\n",
      "Processed batch 4130/5026\n",
      "Processed batch 4140/5026\n",
      "Processed batch 4150/5026\n",
      "Processed batch 4160/5026\n",
      "Processed batch 4170/5026\n",
      "Processed batch 4180/5026\n",
      "Processed batch 4190/5026\n",
      "Processed batch 4200/5026\n",
      "Processed batch 4210/5026\n",
      "Processed batch 4220/5026\n",
      "Processed batch 4230/5026\n",
      "Processed batch 4240/5026\n",
      "Processed batch 4250/5026\n",
      "Processed batch 4260/5026\n",
      "Processed batch 4270/5026\n",
      "Processed batch 4280/5026\n",
      "Processed batch 4290/5026\n",
      "Processed batch 4300/5026\n",
      "Processed batch 4310/5026\n",
      "Processed batch 4320/5026\n",
      "Processed batch 4330/5026\n",
      "Processed batch 4340/5026\n",
      "Processed batch 4350/5026\n",
      "Processed batch 4360/5026\n",
      "Processed batch 4370/5026\n",
      "Processed batch 4380/5026\n",
      "Processed batch 4390/5026\n",
      "Processed batch 4400/5026\n",
      "Processed batch 4410/5026\n",
      "Processed batch 4420/5026\n",
      "Processed batch 4430/5026\n",
      "Processed batch 4440/5026\n",
      "Processed batch 4450/5026\n",
      "Processed batch 4460/5026\n",
      "Processed batch 4470/5026\n",
      "Processed batch 4480/5026\n",
      "Processed batch 4490/5026\n",
      "Processed batch 4500/5026\n",
      "Processed batch 4510/5026\n",
      "Processed batch 4520/5026\n",
      "Processed batch 4530/5026\n",
      "Processed batch 4540/5026\n",
      "Processed batch 4550/5026\n",
      "Processed batch 4560/5026\n",
      "Processed batch 4570/5026\n",
      "Processed batch 4580/5026\n",
      "Processed batch 4590/5026\n",
      "Processed batch 4600/5026\n",
      "Processed batch 4610/5026\n",
      "Processed batch 4620/5026\n",
      "Processed batch 4630/5026\n",
      "Processed batch 4640/5026\n",
      "Processed batch 4650/5026\n",
      "Processed batch 4660/5026\n",
      "Processed batch 4670/5026\n",
      "Processed batch 4680/5026\n",
      "Processed batch 4690/5026\n",
      "Processed batch 4700/5026\n",
      "Processed batch 4710/5026\n",
      "Processed batch 4720/5026\n",
      "Processed batch 4730/5026\n",
      "Processed batch 4740/5026\n",
      "Processed batch 4750/5026\n",
      "Processed batch 4760/5026\n",
      "Processed batch 4770/5026\n",
      "Processed batch 4780/5026\n",
      "Processed batch 4790/5026\n",
      "Processed batch 4800/5026\n",
      "Processed batch 4810/5026\n",
      "Processed batch 4820/5026\n",
      "Processed batch 4830/5026\n",
      "Processed batch 4840/5026\n",
      "Processed batch 4850/5026\n",
      "Processed batch 4860/5026\n",
      "Processed batch 4870/5026\n",
      "Processed batch 4880/5026\n",
      "Processed batch 4890/5026\n",
      "Processed batch 4900/5026\n",
      "Processed batch 4910/5026\n",
      "Processed batch 4920/5026\n",
      "Processed batch 4930/5026\n",
      "Processed batch 4940/5026\n",
      "Processed batch 4950/5026\n",
      "Processed batch 4960/5026\n",
      "Processed batch 4970/5026\n",
      "Processed batch 4980/5026\n",
      "Processed batch 4990/5026\n",
      "Processed batch 5000/5026\n",
      "Processed batch 5010/5026\n",
      "Processed batch 5020/5026\n",
      "Combining predictions...\n",
      "\n",
      "=== Processing predictions ===\n",
      "Prediction shape: (3147, 3194)\n",
      "Prediction min: 0, max: 4\n",
      "Unique classes: [0 1 2 3 4]\n",
      "Class distribution:\n",
      "  Class 0: 7,106,795 pixels (70.70%)\n",
      "  Class 1: 980,613 pixels (9.76%)\n",
      "  Class 2: 930,384 pixels (9.26%)\n",
      "  Class 3: 447,333 pixels (4.45%)\n",
      "  Class 4: 586,393 pixels (5.83%)\n",
      "\n",
      "✓ Prediction saved to: /home/jupyter-bryan/ISA_Data/Raster_02/ISA_DKI_Multi_Orig_withALPE.tif\n",
      "CPU times: user 31min 1s, sys: 1min 55s, total: 32min 57s\n",
      "Wall time: 27min 53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR 1: PROJ: proj_create_from_name: Open of /opt/tljh/user/share/proj failed\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(\"\\n=== Starting prediction with ALPE model ===\")\n",
    "try:\n",
    "    pred_ = predict_in_batches_with_mask(\n",
    "        model, \n",
    "        comb_all, \n",
    "        confidence_mask_final,  # ← Pass masks!\n",
    "        batch_size=2000)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during prediction: {e}\")\n",
    "    print(\"Trying with smaller batch size...\")\n",
    "    try:\n",
    "        pred_ = predict_in_batches_with_mask(\n",
    "            model, \n",
    "            comb_all, \n",
    "            confidence_mask_final,\n",
    "            batch_size=1000)\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"Still getting error: {e2}\")\n",
    "        print(\"Trying CPU-only prediction...\")\n",
    "        with tf.device('/CPU:0'):\n",
    "            pred_ = predict_in_batches_with_mask(\n",
    "                model, \n",
    "                comb_all, \n",
    "                confidence_mask_final,\n",
    "                batch_size=500)\n",
    "\n",
    "print(\"\\n=== Processing predictions ===\")\n",
    "lc_pred_ = pred_.argmax(axis=1)\n",
    "\n",
    "# Reshape back to image dimensions\n",
    "img_pred = np.reshape(lc_pred_, (ds_ndvi.RasterYSize, ds_ndvi.RasterXSize))\n",
    "\n",
    "print(f\"Prediction shape: {img_pred.shape}\")\n",
    "print(f\"Prediction min: {img_pred.min()}, max: {img_pred.max()}\")\n",
    "print(f\"Unique classes: {np.unique(img_pred)}\")\n",
    "print(f\"Class distribution:\")\n",
    "for cls in np.unique(img_pred):\n",
    "    count = (img_pred == cls).sum()\n",
    "    print(f\"  Class {cls}: {count:,} pixels ({100*count/img_pred.size:.2f}%)\")\n",
    "\n",
    "# Export result\n",
    "outFile = r'/home/jupyter-bryan/ISA_Data/Raster_02/ISA_DKI_Multi_Orig_withALPE.tif'\n",
    "raster.export(img_pred, ds_ndvi, filename=outFile, dtype='int')\n",
    "print(f\"\\n✓ Prediction saved to: {outFile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14df945c-3fd8-453c-a5e8-9a2ff3ae52ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
