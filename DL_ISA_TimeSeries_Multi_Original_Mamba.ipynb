{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22166b4c-bcbb-473f-8b92-be0146e49ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 00:34:39.974285: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.backend import sigmoid\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, LayerNormalization, Bidirectional, LSTM, GRU, Layer, SpatialDropout1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Lambda, Reshape, Flatten, Input, MultiHeadAttention, Flatten, Concatenate, Add\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe91c78-81c4-480c-8736-9adf3861ee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_interpolate(data):\n",
    "    df = pd.DataFrame(data).astype(np.float32)\n",
    "    \n",
    "    df_linear = df.interpolate(method='linear', limit_direction='both', axis=1, limit=None)\n",
    "    df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
    "\n",
    "    df_final = df_spline.fillna(0)\n",
    "    \n",
    "    # Clip extreme values\n",
    "    df_interpolated_fin = df_final.clip(-1e6, 1e6)\n",
    "    print(f\"Final NaN count: {df_interpolated_fin.isnull().sum().sum()}\")\n",
    "    result = df_interpolated_fin.values\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69b5bd0e-759a-40c7-bee9-8fc67b2e6604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slope + elevation - fixed constraint\n",
    "\n",
    "def multi_focal_loss_slope_elevation_constraint(slope_values, elevation_values,\n",
    "                                                slope_threshold=40, elevation_threshold=2000,\n",
    "                                                alpha=0.25, gamma=2.0,\n",
    "                                                lambda_slope=0.4, lambda_elevation=0.3):\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred_clipped = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        ce = -y_true * tf.math.log(y_pred_clipped)\n",
    "        p_t = tf.where(tf.equal(y_true, 1), y_pred_clipped, 1 - y_pred_clipped)\n",
    "        \n",
    "        # Alpha weighting for class imbalance\n",
    "        alpha_t = tf.where(tf.equal(y_true[:, 1], 1), alpha, 1 - alpha)\n",
    "        \n",
    "        # Focal loss calculation\n",
    "        focal_weight = tf.expand_dims(alpha_t, 1) * tf.pow((1 - p_t), gamma)\n",
    "        focal_loss = focal_weight * ce\n",
    "        focal_loss = tf.reduce_mean(tf.reduce_sum(focal_loss, axis=1))\n",
    "        \n",
    "        # Get batch data\n",
    "        batch_size = tf.shape(y_pred)[0]\n",
    "        slope_batch = tf.gather(slope_values, tf.range(batch_size))\n",
    "        elevation_batch = tf.gather(elevation_values, tf.range(batch_size))\n",
    "        \n",
    "        # ISA predictions\n",
    "        isa_pred = y_pred[:, 0]\n",
    "        \n",
    "        # CONSTRAINT 1: Slope constraint\n",
    "        slope_mask = tf.cast(slope_batch > slope_threshold, tf.float32)\n",
    "        slope_penalty = tf.reduce_mean(tf.multiply(slope_mask, isa_pred))\n",
    "        \n",
    "        # CONSTRAINT 2: Elevation constraint\n",
    "        elevation_mask = tf.cast(elevation_batch > elevation_threshold, tf.float32)\n",
    "        elevation_penalty = tf.reduce_mean(tf.multiply(elevation_mask, isa_pred))\n",
    "        \n",
    "        # Combine all losses\n",
    "        total_loss = focal_loss + (lambda_slope * slope_penalty) + (lambda_elevation * elevation_penalty)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6425059f-dd63-449a-b124-6cda96d137b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_multi_region_data(citarum_01_paths, citarum_02_paths, citarum_03_paths, jkt_paths):\n",
    "    def load_region_data(paths):\n",
    "        # Load all dataframes for a region\n",
    "        ndvi_df = pd.read_csv(paths['ndvi'],delimiter=';', encoding='utf-8-sig',decimal=',')\n",
    "        mndwi_df = pd.read_csv(paths['mndwi'],delimiter=';', encoding='utf-8-sig',decimal=',')\n",
    "        ndbi_df = pd.read_csv(paths['ndbi'],delimiter=';', encoding='utf-8-sig',decimal=',')\n",
    "        ndbsi_df = pd.read_csv(paths['ndbsi'],delimiter=';', encoding='utf-8-sig',decimal=',')\n",
    "        cbi_df = pd.read_csv(paths['cbi'],delimiter=';', encoding='utf-8-sig',decimal=',')\n",
    "        uci_df = pd.read_csv(paths['uci'],delimiter=';', encoding='utf-8-sig',decimal=',')\n",
    "        print('NDVI shape before int:', ndvi_df.shape)\n",
    "        \n",
    "        # Extract features\n",
    "        ndvi_features = ndvi_df.iloc[:,5:29].values\n",
    "        mndwi_features = mndwi_df.iloc[:,5:29].values\n",
    "        ndbi_features = ndbi_df.iloc[:,5:29].values\n",
    "        ndbsi_features = ndbsi_df.iloc[:,5:29].values\n",
    "        cbi_features = cbi_df.iloc[:,5:29].values\n",
    "        uci_features = uci_df.iloc[:,5:29].values\n",
    "        print('NDVI shape after int:', ndvi_features.shape)\n",
    "        \n",
    "        # Clean and interpolate\n",
    "        ndvi_features = clean_and_interpolate(ndvi_features)\n",
    "        mndwi_features = clean_and_interpolate(mndwi_features)\n",
    "        ndbi_features = clean_and_interpolate(ndbi_features)\n",
    "        ndbsi_features = clean_and_interpolate(ndbsi_features)\n",
    "        cbi_features = clean_and_interpolate(cbi_features)\n",
    "        uci_features = clean_and_interpolate(uci_features)\n",
    "        \n",
    "        # Combine features\n",
    "        X = np.concatenate([ndvi_features, mndwi_features, ndbi_features, \n",
    "                            ndbsi_features, cbi_features, uci_features], axis=1)\n",
    "\n",
    "        print(\"X afer concatenate\", X.shape)\n",
    "        \n",
    "        # Reshape\n",
    "        X = X.reshape(X.shape[0], 24, 6)\n",
    "        print(\"X afer reshape\", X.shape)\n",
    "        \n",
    "        # Get labels, slope and aspect\n",
    "        labels = ndvi_df.iloc[:, 1].values\n",
    "        slope_values = ndvi_df.iloc[:,29].values\n",
    "        aspect_values = ndvi_df.iloc[:,30].values\n",
    "        \n",
    "        # return X, labels, slope_values\n",
    "        return X, labels, slope_values, aspect_values\n",
    "\n",
    "    # Load data for both regions\n",
    "    X_citarum_01, labels_citarum_01, slope_citarum_01, elev_citarum_01 = load_region_data(citarum_01_paths)\n",
    "    X_citarum_02, labels_citarum_02, slope_citarum_02, elev_citarum_02 = load_region_data(citarum_02_paths)\n",
    "    X_citarum_03, labels_citarum_03, slope_citarum_03, elev_citarum_03 = load_region_data(citarum_03_paths)\n",
    "    X_jkt, labels_jkt, slope_jkt, elev_jkt = load_region_data(jkt_paths)\n",
    "    \n",
    "    # Combine data from both regions\n",
    "    X_combined = np.concatenate([X_citarum_01, X_citarum_02, X_citarum_03, X_jkt], axis=0)\n",
    "    labels_combined = np.concatenate([labels_citarum_01, labels_citarum_02, labels_citarum_03, labels_jkt])\n",
    "    slope_combined = np.concatenate([slope_citarum_01, slope_citarum_02, slope_citarum_03, slope_jkt])\n",
    "    elev_combined = np.concatenate([elev_citarum_01, elev_citarum_02, elev_citarum_03, elev_jkt])\n",
    "    \n",
    "    # Compute class weights on combined data\n",
    "    unique_classes = np.unique(labels_combined)\n",
    "\n",
    "    print(\"Labels combined shape:\", labels_combined.shape)\n",
    "    print(\"Labels combined type:\", type(labels_combined[0]) if len(labels_combined) > 0 else \"Empty\")\n",
    "    print(\"Unique classes from np.unique:\", unique_classes)\n",
    "    print(\"Unique classes type:\", type(unique_classes[0]) if len(unique_classes) > 0 else \"Empty\")\n",
    "    print(\"All unique values in labels_combined:\", set(labels_combined))\n",
    "    print(\"Any NaN values?\", np.any(pd.isna(labels_combined)))\n",
    "\n",
    "    # Check if there are any labels in y that aren't in classes\n",
    "    missing_labels = set(labels_combined) - set(unique_classes)\n",
    "    print(\"Labels in y but not in classes:\", missing_labels)\n",
    "    \n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=unique_classes,\n",
    "        y=labels_combined)\n",
    "    \n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print(\"Class weights:\", class_weight_dict)\n",
    "    \n",
    "    y = np.asarray(labels_combined)\n",
    "    print(y)\n",
    "    print(len(np.unique(y)))\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_combined = label_encoder.fit_transform(labels_combined)\n",
    "    y_combined = to_categorical(y_combined)\n",
    "    print(y_combined)\n",
    "    \n",
    "    # Split combined data\n",
    "    X_train, X_test, y_train, y_test, slope_train, slope_test, elev_train, elev_test = train_test_split(X_combined, y_combined,\n",
    "                                                                                                            slope_combined, elev_combined,\n",
    "                                                                                                            test_size=0.3,random_state=42,\n",
    "                                                                                                            stratify=y_combined)\n",
    "    \n",
    "    # Convert slope values to tensorflow constant\n",
    "    slope_train = tf.constant(slope_train, dtype=tf.float32)\n",
    "    slope_test = tf.constant(slope_test, dtype=tf.float32)\n",
    "\n",
    "    # Convert aspect values to tensorflow constant\n",
    "    elev_train = tf.constant(elev_train, dtype=tf.float32)\n",
    "    elev_test = tf.constant(elev_test, dtype=tf.float32)\n",
    "    \n",
    "    return (X_train, X_test, y_train, y_test, label_encoder, class_weight_dict, slope_train, slope_test, elev_train, elev_test, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cced7726-76aa-4c5c-b290-da99967eab35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDVI shape before int: (2473, 31)\n",
      "NDVI shape after int: (2473, 24)\n",
      "Final NaN count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final NaN count: 0\n",
      "Final NaN count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final NaN count: 0\n",
      "Final NaN count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final NaN count: 0\n",
      "X afer concatenate (2473, 144)\n",
      "X afer reshape (2473, 24, 6)\n",
      "NDVI shape before int: (1972, 31)\n",
      "NDVI shape after int: (1972, 24)\n",
      "Final NaN count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final NaN count: 0\n",
      "Final NaN count: 0\n",
      "Final NaN count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final NaN count: 0\n",
      "Final NaN count: 0\n",
      "X afer concatenate (1972, 144)\n",
      "X afer reshape (1972, 24, 6)\n",
      "NDVI shape before int: (533, 31)\n",
      "NDVI shape after int: (533, 24)\n",
      "Final NaN count: 0\n",
      "Final NaN count: 0\n",
      "Final NaN count: 0\n",
      "Final NaN count: 0\n",
      "Final NaN count: 0\n",
      "Final NaN count: 0\n",
      "X afer concatenate (533, 144)\n",
      "X afer reshape (533, 24, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDVI shape before int: (2205, 31)\n",
      "NDVI shape after int: (2205, 24)\n",
      "Final NaN count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final NaN count: 0\n",
      "Final NaN count: 0\n",
      "Final NaN count: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final NaN count: 0\n",
      "Final NaN count: 0\n",
      "X afer concatenate (2205, 144)\n",
      "X afer reshape (2205, 24, 6)\n",
      "Labels combined shape: (7183,)\n",
      "Labels combined type: <class 'numpy.int64'>\n",
      "Unique classes from np.unique: [1 2 3 4 5]\n",
      "Unique classes type: <class 'numpy.int64'>\n",
      "All unique values in labels_combined: {np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5)}\n",
      "Any NaN values? False\n",
      "Labels in y but not in classes: set()\n",
      "Class weights: {0: np.float64(0.4424391746227287), 1: np.float64(0.8934079601990049), 2: np.float64(0.9115482233502538), 3: np.float64(3.1435448577680525), 4: np.float64(4.8698305084745765)}\n",
      "[1 1 1 ... 2 2 2]\n",
      "5\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_606415/3811670575.py:5: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1764434117.196723  606415 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7775 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "citarum_01_paths = {\n",
    "    'ndvi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDVI_ROI_01.csv',\n",
    "    'mndwi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_MNDWI_ROI_01.csv',\n",
    "    'ndbi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDBI_ROI_01.csv',\n",
    "    'ndbsi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDBSI_ROI_01.csv',\n",
    "    'cbi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_CBI_ROI_01.csv',\n",
    "    'uci': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_UCI_ROI_01.csv'\n",
    "}\n",
    "\n",
    "citarum_02_paths = {\n",
    "    'ndvi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDVI_ROI_02.csv',\n",
    "    'mndwi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_MNDWI_ROI_02.csv',\n",
    "    'ndbi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDBI_ROI_02.csv',\n",
    "    'ndbsi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDBSI_ROI_02.csv',\n",
    "    'cbi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_CBI_ROI_02.csv',\n",
    "    'uci': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_UCI_ROI_02.csv'\n",
    "}\n",
    "\n",
    "citarum_03_paths = {\n",
    "    'ndvi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDVI_ROI_03.csv',\n",
    "    'mndwi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_MNDWI_ROI_03.csv',\n",
    "    'ndbi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDBI_ROI_03.csv',\n",
    "    'ndbsi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDBSI_ROI_03.csv',\n",
    "    'cbi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_CBI_ROI_03.csv',\n",
    "    'uci': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_UCI_ROI_03.csv'\n",
    "}\n",
    "\n",
    "jkt_paths = {\n",
    "    'ndvi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_DKI_NDVI.csv',\n",
    "    'mndwi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_DKI_MNDWI.csv',\n",
    "    'ndbi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_DKI_NDBI.csv',\n",
    "    'ndbsi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_DKI_NDBSI.csv',\n",
    "    'cbi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_DKI_CBI.csv',\n",
    "    'uci': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_DKI_UCI.csv'\n",
    "}\n",
    "\n",
    "# Prepare the combined data\n",
    "X_train, X_test, y_train, y_test, label_encoder, class_weight_dict, slope_train, slope_test, elev_train, elev_test, y = prepare_multi_region_data(citarum_01_paths, \n",
    "                                                                                                                                                      citarum_02_paths, \n",
    "                                                                                                                                                      citarum_03_paths,\n",
    "                                                                                                                                                      jkt_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad6ae3f0-91ad-491f-a480-b68918fd0c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaLayer(layers.Layer):\n",
    "    def __init__(self, d_model, d_state=16, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.d_model = d_model\n",
    "        self.d_state = d_state\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # All projections in one go\n",
    "        self.delta_proj = layers.Dense(self.d_state, use_bias=False)\n",
    "        self.B_proj = layers.Dense(self.d_state, use_bias=False) \n",
    "        self.C_proj = layers.Dense(self.d_state, use_bias=False)\n",
    "        \n",
    "        # State matrix A and skip connection D\n",
    "        self.A = self.add_weight(shape=(self.d_model, self.d_state), initializer='random_normal', trainable=True)\n",
    "        self.D = self.add_weight(shape=(self.d_model,), initializer='random_normal', trainable=True)\n",
    "        \n",
    "        super().build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "        \n",
    "        # Generate selective parameters\n",
    "        delta = tf.nn.softplus(self.delta_proj(inputs))  # (batch, seq_len, d_state)\n",
    "        B = self.B_proj(inputs)  # (batch, seq_len, d_state)\n",
    "        C = self.C_proj(inputs)  # (batch, seq_len, d_state)\n",
    "        \n",
    "        # Initialize state\n",
    "        h = tf.zeros((batch_size, self.d_model, self.d_state))\n",
    "        A_matrix = -tf.exp(self.A)  # Stable A matrix\n",
    "        \n",
    "        # Pre-allocate output tensor\n",
    "        outputs = tf.TensorArray(dtype=tf.float32, size=seq_len, dynamic_size=False)\n",
    "        \n",
    "        # Selective scan - process sequence step by step\n",
    "        for t in tf.range(seq_len):\n",
    "            u_t = inputs[:, t, :]  # Current input\n",
    "            delta_t = delta[:, t, :]\n",
    "            B_t = B[:, t, :]\n",
    "            C_t = C[:, t, :]\n",
    "            \n",
    "            # Discretize and update state\n",
    "            A_discrete = tf.exp(tf.expand_dims(delta_t, 1) * tf.expand_dims(A_matrix, 0))\n",
    "            h = A_discrete * h + tf.expand_dims(delta_t, 1) * tf.expand_dims(B_t, 1) * tf.expand_dims(u_t, -1)\n",
    "            \n",
    "            # Generate output\n",
    "            y = tf.reduce_sum(tf.expand_dims(C_t, 1) * h, axis=-1) + self.D * u_t\n",
    "            outputs = outputs.write(t, y)\n",
    "        \n",
    "        # Stack all outputs\n",
    "        return tf.transpose(outputs.stack(), [1, 0, 2])\n",
    "    \n",
    "    def get_config(self):\n",
    "        \"\"\"Required method for serialization\"\"\"\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"d_model\": self.d_model,\n",
    "            \"d_state\": self.d_state,\n",
    "        })\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        \"\"\"Required method for deserialization\"\"\"\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e815f629-ccb2-4d7f-8553-dd6e19d15dbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "Exception encountered when calling MambaLayer.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'mamba_layer' (of type MambaLayer). Either the `MambaLayer.call()` method is incorrect, or you need to implement the `MambaLayer.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by MambaLayer.call():\n  • args=('<KerasTensor shape=(None, 24, 256), dtype=float32, sparse=False, ragged=False, name=keras_tensor_2>',)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperatorNotAllowedInGraphError\u001b[39m            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     16\u001b[39m     output_layer = Dense(n_classes, activation=\u001b[33m'\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m'\u001b[39m)(x)\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Model(inputs=input_layer, outputs=output_layer)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m model = \u001b[43mcreate_mamba_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m model.summary()\n\u001b[32m     23\u001b[39m model.compile(loss=multi_focal_loss_slope_elevation_constraint(\n\u001b[32m     24\u001b[39m                     slope_train, elev_train,  \n\u001b[32m     25\u001b[39m                     slope_threshold=\u001b[32m40\u001b[39m, elevation_threshold=\u001b[32m2000\u001b[39m,\n\u001b[32m     26\u001b[39m                     alpha=\u001b[32m0.25\u001b[39m, gamma=\u001b[32m2.0\u001b[39m, lambda_slope=\u001b[32m0.4\u001b[39m, lambda_elevation=\u001b[32m0.3\u001b[39m),\n\u001b[32m     27\u001b[39m               optimizer=Adam(learning_rate=\u001b[32m0.001\u001b[39m), \n\u001b[32m     28\u001b[39m               metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mcreate_mamba_model\u001b[39m\u001b[34m(n_classes, n_features, n_timesteps, d_model, n_layers)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_layers):\n\u001b[32m     10\u001b[39m     norm_x = LayerNormalization()(x)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     mamba_out = \u001b[43mMambaLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnorm_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     x = tf.keras.layers.add([x, mamba_out])\n\u001b[32m     14\u001b[39m x = Flatten()(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/tljh/user/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mMambaLayer.call\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m     33\u001b[39m outputs = tf.TensorArray(dtype=tf.float32, size=seq_len, dynamic_size=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Selective scan - process sequence step by step\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mu_t\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Current input\u001b[39;49;00m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelta_t\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mOperatorNotAllowedInGraphError\u001b[39m: Exception encountered when calling MambaLayer.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'mamba_layer' (of type MambaLayer). Either the `MambaLayer.call()` method is incorrect, or you need to implement the `MambaLayer.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nIterating over a symbolic `tf.Tensor` is not allowed. You can attempt the following resolutions to the problem: If you are running in Graph mode, use Eager execution mode or decorate this function with @tf.function. If you are using AutoGraph, you can try decorating this function with @tf.function. If that does not work, then you may be using an unsupported feature or your source code may not be visible to AutoGraph. See https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/autograph/g3doc/reference/limitations.md#access-to-source-code for more information.\u001b[0m\n\nArguments received by MambaLayer.call():\n  • args=('<KerasTensor shape=(None, 24, 256), dtype=float32, sparse=False, ragged=False, name=keras_tensor_2>',)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "n_classes=len(label_encoder.classes_)\n",
    "n_features=6\n",
    "\n",
    "def create_mamba_model(n_classes, n_features, n_timesteps=24, d_model=256, n_layers=8):\n",
    "    input_layer = Input(shape=(n_timesteps, n_features))\n",
    "    x = Dense(d_model)(input_layer)\n",
    "    \n",
    "    # Stack Mamba layers with residual connections\n",
    "    for i in range(n_layers):\n",
    "        norm_x = LayerNormalization()(x)\n",
    "        mamba_out = MambaLayer(d_model, d_state=16)(norm_x)\n",
    "        x = tf.keras.layers.add([x, mamba_out])\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    output_layer = Dense(n_classes, activation='softmax')(x)\n",
    "\n",
    "    return Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model = create_mamba_model(n_classes, n_features)\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=multi_focal_loss_slope_elevation_constraint(\n",
    "                    slope_train, elev_train,  \n",
    "                    slope_threshold=40, elevation_threshold=2000,\n",
    "                    alpha=0.25, gamma=2.0, lambda_slope=0.4, lambda_elevation=0.3),\n",
    "              optimizer=Adam(learning_rate=0.001), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='/home/jupyter-bryan/ISA_Data/ISA_Orig_Mamba.h5',\n",
    "                             monitor='val_accuracy',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1,\n",
    "                             mode='max',\n",
    "                             save_weights_only=True)\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_accuracy',\n",
    "                           patience=100,\n",
    "                           restore_best_weights=True,\n",
    "                           mode='max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                              factor=0.1,\n",
    "                              patience=30, \n",
    "                              min_lr=0.00001)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "                    batch_size =20,\n",
    "                    epochs=500, \n",
    "                    callbacks = [early_stop, reduce_lr], \n",
    "                    class_weight = class_weight_dict)\n",
    "\n",
    "model_state = {\n",
    "    'weights': model.get_weights(),\n",
    "    'config': {\n",
    "        'n_classes': n_classes,\n",
    "        'n_features': n_features, \n",
    "        'n_timesteps': 24,\n",
    "        'd_model': 256,\n",
    "        'n_layers': 8\n",
    "    },\n",
    "    'history': history.history\n",
    "}\n",
    "\n",
    "# # Save with pickle\n",
    "# save_path = 'D:/ISA_Citarum/Saved_Model/ISA_Citarum_Multi_Orig_Mamba.pkl'\n",
    "# with open(save_path, 'wb') as f:\n",
    "#     pickle.dump(model_state, f)\n",
    "\n",
    "# print(f\"Model state saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8080da-34ec-4514-83a4-196985a8111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = len(history.history['loss'])\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(n_epochs) #change it based on epoch needed to finish building the model\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
