{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec94391-57bf-45c2-802b-4756bd70d57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "# Force deterministic behavior (will be slower but reproducible)\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f3cd4-1a56-4c9f-af59-d48bc9d332f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.backend import sigmoid\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Dropout, LayerNormalization, Bidirectional, LSTM, GRU, Layer, SpatialDropout1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import Lambda, Reshape, Flatten, Input, MultiHeadAttention, Flatten, Concatenate, Add, Multiply\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38015520-3025-4608-ab3d-e52a95cb34d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8db9fe-21d8-48de-b0f4-0959d59d23db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_interpolate(data):\n",
    "    df = pd.DataFrame(data).astype(np.float32)\n",
    "    \n",
    "    df_linear = df.interpolate(method='linear', limit_direction='both', axis=1, limit=None)\n",
    "    df_spline = df_linear.interpolate(method='spline', order=3, axis=1).fillna(method='ffill', axis=1).fillna(method='bfill', axis=1)\n",
    "\n",
    "    df_final = df_spline.fillna(0)\n",
    "    \n",
    "    # Clip extreme values\n",
    "    df_interpolated_fin = df_final.clip(-1e6, 1e6)\n",
    "    print(f\"Final NaN count: {df_interpolated_fin.isnull().sum().sum()}\")\n",
    "    result = df_interpolated_fin.values\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a42348f-848d-45e9-b131-ee72348d2b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slope + elevation - fixed constraint\n",
    "\n",
    "def multi_focal_loss_slope_elevation_constraint(slope_values, elevation_values,\n",
    "                                                slope_threshold=40, elevation_threshold=2000,\n",
    "                                                alpha=0.25, gamma=2.0,\n",
    "                                                lambda_slope=0.4, lambda_elevation=0.3):\n",
    "    def loss(y_true, y_pred):\n",
    "        epsilon = tf.keras.backend.epsilon()\n",
    "        y_pred_clipped = tf.clip_by_value(y_pred, epsilon, 1.0 - epsilon)\n",
    "        ce = -y_true * tf.math.log(y_pred_clipped)\n",
    "        p_t = tf.where(tf.equal(y_true, 1), y_pred_clipped, 1 - y_pred_clipped)\n",
    "        \n",
    "        # Alpha weighting for class imbalance\n",
    "        alpha_t = tf.where(tf.equal(y_true[:, 1], 1), alpha, 1 - alpha)\n",
    "        \n",
    "        # Focal loss calculation\n",
    "        focal_weight = tf.expand_dims(alpha_t, 1) * tf.pow((1 - p_t), gamma)\n",
    "        focal_loss = focal_weight * ce\n",
    "        focal_loss = tf.reduce_mean(tf.reduce_sum(focal_loss, axis=1))\n",
    "        \n",
    "        # Get batch data\n",
    "        batch_size = tf.shape(y_pred)[0]\n",
    "        slope_batch = tf.gather(slope_values, tf.range(batch_size))\n",
    "        elevation_batch = tf.gather(elevation_values, tf.range(batch_size))\n",
    "        \n",
    "        # ISA predictions\n",
    "        isa_pred = y_pred[:, 0]\n",
    "        \n",
    "        # CONSTRAINT 1: Slope constraint\n",
    "        slope_mask = tf.cast(slope_batch > slope_threshold, tf.float32)\n",
    "        slope_penalty = tf.reduce_mean(tf.multiply(slope_mask, isa_pred))\n",
    "        \n",
    "        # CONSTRAINT 2: Elevation constraint\n",
    "        elevation_mask = tf.cast(elevation_batch > elevation_threshold, tf.float32)\n",
    "        elevation_penalty = tf.reduce_mean(tf.multiply(elevation_mask, isa_pred))\n",
    "        \n",
    "        # Combine all losses\n",
    "        total_loss = focal_loss + (lambda_slope * slope_penalty) + (lambda_elevation * elevation_penalty)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a96bc9-21ce-40cd-a0c7-6e7c3d0705cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_multi_region_data(citarum_01_paths, citarum_02_paths, citarum_03_paths, jkt_paths):\n",
    "    def load_region_data(paths):\n",
    "        # Load all dataframes for a region\n",
    "        ndvi_df = pd.read_csv(paths['ndvi'],delimiter=';', encoding='utf-8-sig',decimal=',')\n",
    "        mndwi_df = pd.read_csv(paths['mndwi'],delimiter=';', encoding='utf-8-sig',decimal=',')\n",
    "        ndbi_df = pd.read_csv(paths['ndbi'],delimiter=';', encoding='utf-8-sig',decimal=',')\n",
    "        ndbsi_df = pd.read_csv(paths['ndbsi'],delimiter=';', encoding='utf-8-sig',decimal=',')\n",
    "        cbi_df = pd.read_csv(paths['cbi'],delimiter=';', encoding='utf-8-sig',decimal=',')\n",
    "        uci_df = pd.read_csv(paths['uci'],delimiter=';', encoding='utf-8-sig',decimal=',')\n",
    "        print('NDVI shape before int:', ndvi_df.shape)\n",
    "        \n",
    "        # Extract features\n",
    "        ndvi_features = ndvi_df.iloc[:,5:29].values\n",
    "        mndwi_features = mndwi_df.iloc[:,5:29].values\n",
    "        ndbi_features = ndbi_df.iloc[:,5:29].values\n",
    "        ndbsi_features = ndbsi_df.iloc[:,5:29].values\n",
    "        cbi_features = cbi_df.iloc[:,5:29].values\n",
    "        uci_features = uci_df.iloc[:,5:29].values\n",
    "        print('NDVI shape after int:', ndvi_features.shape)\n",
    "        \n",
    "        # Clean and interpolate\n",
    "        ndvi_features = clean_and_interpolate(ndvi_features)\n",
    "        mndwi_features = clean_and_interpolate(mndwi_features)\n",
    "        ndbi_features = clean_and_interpolate(ndbi_features)\n",
    "        ndbsi_features = clean_and_interpolate(ndbsi_features)\n",
    "        cbi_features = clean_and_interpolate(cbi_features)\n",
    "        uci_features = clean_and_interpolate(uci_features)\n",
    "        \n",
    "        # Combine features\n",
    "        X = np.concatenate([ndvi_features, mndwi_features, ndbi_features, \n",
    "                            ndbsi_features, cbi_features, uci_features], axis=1)\n",
    "\n",
    "        print(\"X afer concatenate\", X.shape)\n",
    "        \n",
    "        # Reshape\n",
    "        X = X.reshape(X.shape[0], 24, 6)\n",
    "        print(\"X afer reshape\", X.shape)\n",
    "        \n",
    "        # Get labels, slope and aspect\n",
    "        labels = ndvi_df.iloc[:, 1].values\n",
    "        slope_values = ndvi_df.iloc[:,29].values\n",
    "        elev_values = ndvi_df.iloc[:,30].values\n",
    "        \n",
    "        # return X, labels, slope_values\n",
    "        return X, labels, slope_values, elev_values\n",
    "\n",
    "    # Load data for both regions\n",
    "    X_citarum_01, labels_citarum_01, slope_citarum_01, elev_citarum_01 = load_region_data(citarum_01_paths)\n",
    "    X_citarum_02, labels_citarum_02, slope_citarum_02, elev_citarum_02 = load_region_data(citarum_02_paths)\n",
    "    X_citarum_03, labels_citarum_03, slope_citarum_03, elev_citarum_03 = load_region_data(citarum_03_paths)\n",
    "    X_jkt, labels_jkt, slope_jkt, elev_jkt = load_region_data(jkt_paths)\n",
    "    \n",
    "    # Combine data from both regions\n",
    "    X_combined = np.concatenate([X_citarum_01, X_citarum_02, X_citarum_03, X_jkt], axis=0)\n",
    "    labels_combined = np.concatenate([labels_citarum_01, labels_citarum_02, labels_citarum_03, labels_jkt])\n",
    "    slope_combined = np.concatenate([slope_citarum_01, slope_citarum_02, slope_citarum_03, slope_jkt])\n",
    "    elev_combined = np.concatenate([elev_citarum_01, elev_citarum_02, elev_citarum_03, elev_jkt])\n",
    "    \n",
    "    # Compute class weights on combined data\n",
    "    unique_classes = np.unique(labels_combined)\n",
    "\n",
    "    print(\"Labels combined shape:\", labels_combined.shape)\n",
    "    print(\"Labels combined type:\", type(labels_combined[0]) if len(labels_combined) > 0 else \"Empty\")\n",
    "    print(\"Unique classes from np.unique:\", unique_classes)\n",
    "    print(\"Unique classes type:\", type(unique_classes[0]) if len(unique_classes) > 0 else \"Empty\")\n",
    "    print(\"All unique values in labels_combined:\", set(labels_combined))\n",
    "    print(\"Any NaN values?\", np.any(pd.isna(labels_combined)))\n",
    "\n",
    "    # Check if there are any labels in y that aren't in classes\n",
    "    missing_labels = set(labels_combined) - set(unique_classes)\n",
    "    print(\"Labels in y but not in classes:\", missing_labels)\n",
    "    \n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=unique_classes,\n",
    "        y=labels_combined)\n",
    "    \n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print(\"Class weights:\", class_weight_dict)\n",
    "    \n",
    "    y = np.asarray(labels_combined)\n",
    "    print(y)\n",
    "    print(len(np.unique(y)))\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_combined = label_encoder.fit_transform(labels_combined)\n",
    "    y_combined = to_categorical(y_combined)\n",
    "    print(y_combined)\n",
    "    \n",
    "    # Split combined data\n",
    "    X_train, X_test, y_train, y_test, slope_train, slope_test, elev_train, elev_test = train_test_split(X_combined, y_combined,\n",
    "                                                                                                            slope_combined, elev_combined,\n",
    "                                                                                                            test_size=0.3,random_state=42,\n",
    "                                                                                                            stratify=y_combined)\n",
    "    \n",
    "    # Convert slope values to tensorflow constant\n",
    "    slope_train = tf.constant(slope_train, dtype=tf.float32)\n",
    "    slope_test = tf.constant(slope_test, dtype=tf.float32)\n",
    "\n",
    "    # Convert aspect values to tensorflow constant\n",
    "    elev_train = tf.constant(elev_train, dtype=tf.float32)\n",
    "    elev_test = tf.constant(elev_test, dtype=tf.float32)\n",
    "    \n",
    "    return (X_train, X_test, y_train, y_test, label_encoder, class_weight_dict, slope_train, slope_test, elev_train, elev_test, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d6405-f526-4849-827c-b27981744750",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "citarum_01_paths = {\n",
    "    'ndvi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDVI_ROI_01.csv',\n",
    "    'mndwi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_MNDWI_ROI_01.csv',\n",
    "    'ndbi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDBI_ROI_01.csv',\n",
    "    'ndbsi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDBSI_ROI_01.csv',\n",
    "    'cbi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_CBI_ROI_01.csv',\n",
    "    'uci': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_UCI_ROI_01.csv'\n",
    "}\n",
    "\n",
    "citarum_02_paths = {\n",
    "    'ndvi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDVI_ROI_02.csv',\n",
    "    'mndwi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_MNDWI_ROI_02.csv',\n",
    "    'ndbi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDBI_ROI_02.csv',\n",
    "    'ndbsi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDBSI_ROI_02.csv',\n",
    "    'cbi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_CBI_ROI_02.csv',\n",
    "    'uci': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_UCI_ROI_02.csv'\n",
    "}\n",
    "\n",
    "citarum_03_paths = {\n",
    "    'ndvi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDVI_ROI_03.csv',\n",
    "    'mndwi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_MNDWI_ROI_03.csv',\n",
    "    'ndbi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDBI_ROI_03.csv',\n",
    "    'ndbsi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_NDBSI_ROI_03.csv',\n",
    "    'cbi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_CBI_ROI_03.csv',\n",
    "    'uci': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_Citarum_UCI_ROI_03.csv'\n",
    "}\n",
    "\n",
    "jkt_paths = {\n",
    "    'ndvi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_DKI_NDVI.csv',\n",
    "    'mndwi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_DKI_MNDWI.csv',\n",
    "    'ndbi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_DKI_NDBI.csv',\n",
    "    'ndbsi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_DKI_NDBSI.csv',\n",
    "    'cbi': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_DKI_CBI.csv',\n",
    "    'uci': '/home/jupyter-bryan/ISA_Data/Sample_Points_ISA_DKI_UCI.csv'\n",
    "}\n",
    "\n",
    "# Prepare the combined data\n",
    "X_train, X_test, y_train, y_test, label_encoder, class_weight_dict, slope_train, slope_test, elev_train, elev_test, y = prepare_multi_region_data(citarum_01_paths, \n",
    "                                                                                                                                                      citarum_02_paths, \n",
    "                                                                                                                                                      citarum_03_paths,\n",
    "                                                                                                                                                      jkt_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d475ec5-f58e-49df-994d-65ca093b4770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish(x, beta = 1):\n",
    "    return (x * sigmoid(beta * x))\n",
    "\n",
    "tf.keras.utils.get_custom_objects().update({'swish': swish})\n",
    "\n",
    "def ReshapeLayer(x):\n",
    "    shape = x.shape\n",
    "    # 1 possibility: H,W*channel\n",
    "    reshape = Reshape((shape[1],shape[2]))(x)\n",
    "    # 2 possibility: W,H*channel\n",
    "    # transpose = Permute((2,1,3))(x)\n",
    "    # reshape = Reshape((shape[1],shape[2]*shape[3]))(transpose)\n",
    "    return reshape\n",
    "\n",
    "# def self_attention_block(x, dim):\n",
    "#     q = Dense(dim)(x)\n",
    "#     k = Dense(dim)(x)\n",
    "#     v = Dense(dim)(x)\n",
    "#     scores = tf.matmul(q, k, transpose_b=True)\n",
    "#     attention_weights = tf.nn.softmax(scores / tf.sqrt(tf.cast(dim, tf.float32)))\n",
    "    \n",
    "#     return tf.matmul(attention_weights, v)\n",
    "\n",
    "# def self_attention_block(x, dim):\n",
    "#     q = Dense(dim)(x)  # (batch, seq_len, dim)\n",
    "#     k = Dense(dim)(x)  # (batch, seq_len, dim)\n",
    "#     v = Dense(dim)(x)  # (batch, seq_len, dim)\n",
    "    \n",
    "#     # Compute attention scores with proper batch handling\n",
    "#     scores = tf.matmul(q, k, transpose_b=True)  # (batch, seq_len, seq_len)\n",
    "#     scores = scores / tf.sqrt(tf.cast(dim, tf.float32))\n",
    "    \n",
    "#     attention_weights = tf.nn.softmax(scores, axis=-1)  # (batch, seq_len, seq_len)\n",
    "    \n",
    "#     output = tf.matmul(attention_weights, v)  # (batch, seq_len, dim)\n",
    "    \n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42dedfc-46ef-4c6e-9ed6-1f9f0efa20d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_classes=len(label_encoder.classes_)\n",
    "\n",
    "def make_model(n_classes, n_timesteps=24, n_features=6):\n",
    "    input_layer = Input(shape=(n_timesteps, n_features))\n",
    "    \n",
    "    x0a = Conv1D(16, 3, activation=\"swish\", padding = 'same')(input_layer)\n",
    "    x01a = Conv1D(32, 3, activation=\"swish\", padding = 'same')(x0a)\n",
    "    x01a = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x01a)\n",
    "    \n",
    "    x02a = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x01a)\n",
    "    x02a = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x02a)\n",
    "\n",
    "    x03a = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x02a)\n",
    "    x03a = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x03a)\n",
    "\n",
    "    xa = tf.keras.layers.add([x01a, x03a])\n",
    "\n",
    "    x0b = Conv1D(16, 3, activation=\"swish\", padding = 'same')(input_layer)\n",
    "    x01b = Conv1D(32, 3, activation=\"swish\", padding = 'same')(x0b)\n",
    "    x01b = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x01b)\n",
    "    \n",
    "    x02b = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x01b)\n",
    "    x02b = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x02b)\n",
    "\n",
    "    x03b = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x02b)\n",
    "    x03b = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x03b)\n",
    "\n",
    "    xb = tf.keras.layers.add([x01b, x03b])\n",
    "\n",
    "    conc_ = Concatenate()([xa, xb])\n",
    "\n",
    "    x0c = Conv1D(64, 3, activation=\"relu\", padding = 'same')(conc_)\n",
    "    x0c = Conv1D(64, 3, activation=\"relu\", padding = 'same')(x0c)\n",
    "    x0c = LayerNormalization()(x0c)\n",
    "    # x0b = SpatialDropout1D(0.25)(x0b)\n",
    "    \n",
    "    x_ft = Flatten()(x0c)\n",
    "    \n",
    "#     attention = MultiHeadAttention(num_heads=8, key_dim=64)(xab, xab, xab)\n",
    "#     attention = Dropout(0.5)(attention)\n",
    "#     attention = LayerNormalization()(attention + xab)\n",
    "    \n",
    "#     xfin = Bidirectional(GRU(64, activation='tanh', return_sequences=False))(attention)\n",
    "#     xfin = Dropout(0.5)(xfin)\n",
    "    \n",
    "#     xfin = Dense(128, activation='swish')(xfin)\n",
    "#     xfin = Dropout(0.5)(xfin)\n",
    "\n",
    "#     x_att = swin_transformer_block(xab, embed_dim=64, num_heads=8, \n",
    "#                                     window_size=6,depth=2)\n",
    "\n",
    "#     x_att = self_attention_block(xab, 64)\n",
    "\n",
    "#     x_tcn = tcn_block(xab, num_levels=4,filters=64,\n",
    "#                       kernel_size=3,\n",
    "#                       dropout_rate=0.2)\n",
    "    \n",
    "#     x_tcn = tcn_sequence(input_layer, filter_number=64, kernel_size=3, num_blocks=4)\n",
    "#     x_tcn = Flatten()(x_tcn)\n",
    "#     x_tcn = Dropout(0.5)(x_tcn)\n",
    "    \n",
    "    # x_att = self_attention_block(conc_, 32)\n",
    "    # x_att = Flatten()(x_att)\n",
    "    \n",
    "    x_gru = Bidirectional(GRU(32, activation='tanh', return_sequences=True))(conc_)\n",
    "    x_gru = Bidirectional(GRU(64, activation='tanh', return_sequences=False))(x_gru)\n",
    "    x_gru = LayerNormalization()(x_gru)\n",
    "    \n",
    "    x_conc = Concatenate()([x_gru, x_ft])\n",
    "    x_conc = Dense(64, activation='relu')(x_conc)\n",
    "    \n",
    "    output_layer = Dense(n_classes, activation=\"softmax\")(x_conc)\n",
    "\n",
    "    return Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model = make_model(n_classes=len(label_encoder.classes_))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss=multi_focal_loss_slope_elevation_constraint(\n",
    "                    slope_train, elev_train,  \n",
    "                    slope_threshold=40, elevation_threshold=2000,\n",
    "                    alpha=0.25, gamma=2.0, lambda_slope=0.4, lambda_elevation=0.3),\n",
    "              optimizer=Adam(learning_rate=0.001), \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "#               optimizer=Adam(learning_rate=0.001), \n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss=custom_loss_with_slope_constraint(\n",
    "#                     slope_train, \n",
    "#                     slope_threshold=40),\n",
    "#               optimizer=Adam(learning_rate=0.001), \n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath='/home/jupyter-bryan/ISA_Data/ISA_Citarum_Multi_Orig.keras',\n",
    "                             monitor='val_accuracy',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1,\n",
    "                             mode='max')\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_accuracy',\n",
    "                           patience=100,\n",
    "                           restore_best_weights=True,\n",
    "                           mode='max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "                              factor=0.1,\n",
    "                              patience=30, \n",
    "                              min_lr=0.00001)\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "                    batch_size =20,\n",
    "                    epochs=500, \n",
    "                    callbacks = [checkpoint, early_stop, reduce_lr], \n",
    "                    class_weight = class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d75857a-810a-4084-896c-d142f3d86260",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# n_classes=len(label_encoder.classes_)\n",
    "\n",
    "# def make_model(n_classes, n_timesteps=24, n_features=6):\n",
    "#     input_layer = Input(shape=(n_timesteps, n_features))\n",
    "    \n",
    "#     x0a = Conv1D(16, 3, activation=\"swish\", padding = 'same')(input_layer)\n",
    "#     x01a = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x0a)\n",
    "#     x01a = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x01a)\n",
    "    \n",
    "#     x02a = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x01a)\n",
    "#     x02a = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x02a)\n",
    "    \n",
    "#     x03a = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x02a)\n",
    "#     x03a = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x03a)\n",
    "\n",
    "#     xa = tf.keras.layers.add([x01a, x03a])\n",
    "\n",
    "#     x0b = Conv1D(16, 3, activation=\"swish\", padding = 'same')(input_layer)\n",
    "#     x01b = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x0b)\n",
    "#     x01b = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x01b)\n",
    "    \n",
    "#     x02b = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x01b)\n",
    "#     x02b = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x02b)\n",
    "    \n",
    "#     x03b = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x02b)\n",
    "#     x03b = Conv1D(64, 3, activation=\"swish\", padding = 'same')(x03b)\n",
    "\n",
    "#     xb = tf.keras.layers.add([x01b, x03b])\n",
    "\n",
    "#     conc_ = Concatenate()([xa, xb])\n",
    "#     conc_ = Dropout(0.5)(conc_)\n",
    "\n",
    "#     x0c = Conv1D(64, 3, activation=\"relu\", padding = 'same')(conc_)\n",
    "#     x0c = Conv1D(64, 3, activation=\"relu\", padding = 'same')(x0c)\n",
    "#     x0c = LayerNormalization()(x0c)\n",
    "    \n",
    "#     x_ft = Flatten()(x0c)\n",
    "    \n",
    "#     x_gru = Bidirectional(GRU(32, activation='tanh', return_sequences=True))(conc_)\n",
    "#     x_gru = Bidirectional(GRU(64, activation='tanh', return_sequences=False))(x_gru)\n",
    "#     x_gru = LayerNormalization()(x_gru)\n",
    "    \n",
    "#     x_conc = Concatenate()([x_gru, x_ft])\n",
    "#     x_conc = Dropout(0.3)(x_conc)\n",
    "#     x_conc = Dense(64, activation='relu')(x_conc)\n",
    "#     x_conc = Dropout(0.3)(x_conc)\n",
    "    \n",
    "#     output_layer = Dense(n_classes, activation=\"softmax\")(x_conc)\n",
    "\n",
    "#     return Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# model = make_model(n_classes=len(label_encoder.classes_))\n",
    "# model.summary()\n",
    "\n",
    "# model.compile(loss=multi_focal_loss_slope_elevation_constraint(\n",
    "#                     slope_train, elev_train,  \n",
    "#                     slope_threshold=40, elevation_threshold=2000,\n",
    "#                     alpha=0.25, gamma=2.0, lambda_slope=0.4, lambda_elevation=0.3),\n",
    "#               optimizer=Adam(learning_rate=0.001), \n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# # model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "# #               optimizer=Adam(learning_rate=0.001), \n",
    "# #               metrics=['accuracy'])\n",
    "\n",
    "# checkpoint = ModelCheckpoint(filepath='/home/jupyter-bryan/ISA_Data/ISA_Citarum_Multi_Orig_02.h5',\n",
    "#                              monitor='val_accuracy',\n",
    "#                              save_best_only=True,\n",
    "#                              verbose=1,\n",
    "#                              mode='max')\n",
    "\n",
    "# early_stop = EarlyStopping(monitor='val_accuracy',\n",
    "#                            patience=100,\n",
    "#                            restore_best_weights=True,\n",
    "#                            mode='max')\n",
    "\n",
    "# reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', \n",
    "#                               factor=0.1,\n",
    "#                               patience=30, \n",
    "#                               min_lr=0.00001)\n",
    "\n",
    "# history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "#                     batch_size =20,\n",
    "#                     epochs=500, \n",
    "#                     callbacks = [checkpoint, early_stop, reduce_lr], \n",
    "#                     class_weight = class_weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db86f38a-5a64-4629-b76f-b5b1934fd921",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train dtype:\", X_train.dtype)\n",
    "print(\"X_test dtype:\", X_test.dtype)\n",
    "print(\"y_train dtype:\", y_train.dtype)\n",
    "print(\"y_test dtype:\", y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b7bb06-52e7-4c7b-b7b2-1bea3ee587ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = len(history.history['loss'])\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(n_epochs) #change it based on epoch needed to finish building the model\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef01919-bbb1-4c5a-81de-4b829f0fb21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classes = ['ISA','Dense Vegetation','Less Dense Vegetation', 'Bareland', 'Waterbody']\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "score = accuracy_score(y_test, y_pred)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "dl_cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "im = ax.imshow(dl_cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "ax.set(xticks=np.arange(dl_cm.shape[1]),\n",
    "        yticks=np.arange(dl_cm.shape[0]),\n",
    "        # ... and label them with the respective list entries\n",
    "        xticklabels=classes, yticklabels=classes,\n",
    "        title='Normalized Confusion Matrix',\n",
    "        ylabel='True label',\n",
    "        xlabel='Predicted label')\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "          rotation_mode=\"anchor\")\n",
    "\n",
    "fmt = '.2f'\n",
    "thresh = dl_cm.max() / 2.\n",
    "for i in range(dl_cm.shape[0]):\n",
    "    for j in range(dl_cm.shape[1]):\n",
    "        ax.text(j, i, format(dl_cm[i, j], fmt),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if dl_cm[i, j] > thresh else \"black\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e77701-26cb-4579-9d1e-602428d52e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def permutation_importance_predict(model, X_test, y_test, feature_names, n_repeats=5):\n",
    "    X_test = np.array(X_test, dtype=np.float32)\n",
    "    \n",
    "    # Debug: Check shapes\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "    \n",
    "    # Get baseline predictions\n",
    "    baseline_pred = model.predict(X_test, verbose=0)\n",
    "    print(f\"baseline_pred shape: {baseline_pred.shape}\")\n",
    "    \n",
    "    # Handle different y_test formats\n",
    "    if len(y_test.shape) == 1:\n",
    "        # If y_test is 1D (class indices), use it directly\n",
    "        y_true_classes = y_test\n",
    "    else:\n",
    "        # If y_test is one-hot encoded, convert to class indices\n",
    "        y_true_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Convert predictions to class indices\n",
    "    baseline_pred_classes = np.argmax(baseline_pred, axis=1)\n",
    "    \n",
    "    baseline_acc = np.mean(baseline_pred_classes == y_true_classes)\n",
    "    print(f\"Baseline accuracy: {baseline_acc:.4f}\")\n",
    "    \n",
    "    importance_scores = {}\n",
    "    \n",
    "    for i, feature_name in enumerate(feature_names):\n",
    "        print(f\"Computing importance for {feature_name}...\")\n",
    "        \n",
    "        scores = []\n",
    "        for repeat in range(n_repeats):\n",
    "            X_test_perm = np.copy(X_test).astype(np.float32)\n",
    "            \n",
    "            # Permute feature i across all time steps\n",
    "            for t in range(X_test.shape[1]):\n",
    "                np.random.shuffle(X_test_perm[:, t, i])\n",
    "            \n",
    "            # Use predict instead of evaluate\n",
    "            perm_pred = model.predict(X_test_perm, verbose=0)\n",
    "            perm_pred_classes = np.argmax(perm_pred, axis=1)\n",
    "            perm_acc = np.mean(perm_pred_classes == y_true_classes)\n",
    "            importance_drop = baseline_acc - perm_acc\n",
    "            scores.append(importance_drop)\n",
    "        \n",
    "        importance_scores[feature_name] = {\n",
    "            'mean': np.mean(scores),\n",
    "            'std': np.std(scores),\n",
    "            'scores': scores\n",
    "        }\n",
    "        \n",
    "        print(f\"{feature_name}: {np.mean(scores):.4f} ± {np.std(scores):.4f}\")\n",
    "    \n",
    "    return importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cccc50-c14f-4b09-bf30-31453c0ae733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(importance_scores):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    features = list(importance_scores.keys())\n",
    "    means = [importance_scores[f]['mean'] for f in features]\n",
    "    stds = [importance_scores[f]['std'] for f in features]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(features, means, yerr=stds, capsize=5, alpha=0.7)\n",
    "    plt.ylabel('Importance Score (Accuracy Drop)')\n",
    "    plt.title('Feature Importance (Permutation-based)')\n",
    "    plt.xticks(rotation=0)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, mean in zip(bars, means):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002, \n",
    "                f'{mean:.3f}', ha='center', va='bottom')\n",
    "\n",
    "    plt.margins(x=0.01)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b88358-7eaa-488d-a706-6d8775835f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = ['NDVI', 'MNDWI', 'NDBI', 'NDBSI', 'CBI', 'UCI']\n",
    "importance_results = permutation_importance_predict(model, X_test, y_test, feature_names, n_repeats=5)\n",
    "\n",
    "plot_feature_importance(importance_results)\n",
    "sorted_features = sorted(importance_results.items(), key=lambda x: x[1]['mean'], reverse=True)\n",
    "print(\"\\nFeature Importance Ranking:\")\n",
    "for i, (feature, scores) in enumerate(sorted_features, 1):\n",
    "    print(f\"{i}. {feature}: {scores['mean']:.4f} ± {scores['std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f992b430-a810-4e7c-8b9d-0a909ea98152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
